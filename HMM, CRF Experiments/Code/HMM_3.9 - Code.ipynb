{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02e219-fc9d-4d01-895c-a96328258294",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMPDEL 2 HMM GMM 3.9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "from enum import Enum, auto  \n",
    "import warnings\n",
    "\n",
    "\n",
    "\"\"\" --- OLD ! \n",
    "'helix': np.array([0.52, 0.30, 0.18]),  # Helix mixing proportions\n",
    "'sheet': np.array([0.38, 0.34, 0.28]),  # Sheet mixing proportions\n",
    "'coil': np.array([0.34, 0.33, 0.33])    # Coil mixing proportions\n",
    "\n",
    "# # Emission configuration from structure analysis --- outdated and has mistakes.\n",
    "# self.emission_config = {\n",
    "#     'base_std': 0.285,        # From PSSM analysis\n",
    "#     'init_noise_scale': 0.05,  # Reduce for better stability\n",
    "#     'state_biases': {\n",
    "#         'helix': [0.46, 0.35, 0.19],  # Match mixture weights\n",
    "#         'sheet': [0.46, 0.35, 0.19],  # Keep consistent\n",
    "#         'coil': [0.46, 0.35, 0.19]    # Use analyzed proportions\n",
    "#     },\n",
    "#     'update_clip': 0.1        # For gradient stability\n",
    "# }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'protein_structure_pred_{datetime.now():%Y%m%d_%H%M}.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## auto() function is used in Enums to automatically assign incrementing values to enum members.\n",
    "\n",
    "class StructureState(Enum):\n",
    "    \"\"\"Three-state DSSP classification\"\"\"\n",
    "    HELIX = auto()  # H: Alpha helix, 3-10 helix, Pi helix\n",
    "    SHEET = auto()  # E: Extended strand, Bridge\n",
    "    COIL = auto()   # C: Coil, Turn, Bend\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "\n",
    "        # Core parameters\n",
    "        self.n_states = 3    # ✓ From structure classification\n",
    "        self.n_mixtures = 3  # ✓ From mixture analysis (optimal clusters)\n",
    "        self.n_features = 46    # ✓ From feature space analysis\n",
    "        self.random_seed = 42  # ⚠️ Still arbitrary\n",
    "        \n",
    "        # # Training parameters (existing)\n",
    "        # self.learning_rate = 0.005 # self.lr_decay = 0.99 # self.clip_value = 2.0 # self.momentum = 0.1 # self.batch_size = 32\n",
    "        \n",
    "        # Training parameters (updated from analysis)\n",
    "        self.learning_rate = 0.094353  # From feature variance analysis\n",
    "        self.lr_decay = 0.9791        # From feature stability\n",
    "        self.clip_value = 0.5871      # From gradient analysis\n",
    "        self.momentum = 0.0941        # From autocorrelation analysis\n",
    "        self.batch_size = 32          # Keep current as analysis suggests very small batches\n",
    "    \n",
    "        # State balance parameters (updated from distribution analysis)\n",
    "        self.min_state_prob = 0.016   # From state distribution\n",
    "        self.max_state_prob = 0.047   # From state distribution\n",
    "        self.min_mixture_prob = 0.1   # Keep current as analysis supports this\n",
    "        self.state_balance_weight = 0.5\n",
    "    \n",
    "        # Target distribution (keep current as validated by position analysis)\n",
    "        self.target_state_dist = np.array([0.492, 0.162, 0.346]) # ✓ From NPY analysis # Helix, Sheet, Coil\n",
    "\n",
    "        \"\"\" suggested set:\n",
    "            'transition_params': {\n",
    "            'helix_self': 0.8,      # Reduced from 0.91 for better stability\n",
    "            'sheet_self': 0.6,      # Reduced from 0.67\n",
    "            'coil_self': 0.4,       # Slightly increased from 0.39\n",
    "            'helix_to_sheet': 0.15,\n",
    "            'sheet_to_coil': 0.15,\n",
    "            'coil_to_helix': 0.15   # Made mixing probabilities equal\n",
    "        },\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialization parameters from detailed NPY analysis\n",
    "        self.init_params = {\n",
    "            'state_priors': {\n",
    "                'helix': 0.492,  # From position-wise structure counting\n",
    "                'sheet': 0.162,  # Combined E+B states frequency\n",
    "                'coil': 0.346   # Combined T+S+C states frequency\n",
    "            },\n",
    "            'transition_params': {\n",
    "                'helix_self': 0.91,   # Direct H→H transition probability\n",
    "                'sheet_self': 0.67,   # Measured from G→G transitions\n",
    "                'coil_self': 0.39,    # Actual C→C transition rate\n",
    "                'helix_to_sheet': 0.15,  # Measured H→E transition\n",
    "                'sheet_to_coil': 0.18,   # Measured E→C transition\n",
    "                'coil_to_helix': 0.10    # Measured C→H transition\n",
    "            },\n",
    "            'emission_params': {\n",
    "                'mean_scale': 0.136,    # From PSSM analysis\n",
    "                'std_scale': 0.285,     # From feature std analysis\n",
    "                'noise_scale': 0.1782,  # From mixture analysis\n",
    "                \n",
    "                'helix_boost': 1.2,      # Based on higher helix conservation # ✓ From conservation patterns\n",
    "                'sheet_boost': 1.0,      # From moderate sheet conservation # ✓ From conservation patterns\n",
    "                'coil_boost': 0.8        # Reflects higher coil variability # ✓ From conservation patterns\n",
    "            },\n",
    "            'feature_weights': {\n",
    "                'one_hot': 0.42,     # Measured primary structure influence    # ✓ From feature importance analysis\n",
    "                'pssm': 0.39,        # Measured evolutionary signal impact    # ✓ From feature importance analysis\n",
    "                'aux': 0.19          # Measured auxiliary property contribution     # ✓ From feature importance analysis\n",
    "            },\n",
    "            'mixture_weights': {\n",
    "                'primary': 0.46,     # Dominant signal component        # ✓ From mixture analysis\n",
    "                'secondary': 0.35,   # Supporting signal strength          # ✓ From mixture analysis\n",
    "                'tertiary': 0.19     # Refinement signal proportion          # ✓ From mixture analysis\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Feature configuration\n",
    "        self.feature_config = {\n",
    "            'one_hot': {\n",
    "                'start': 0, 'end': 21,\n",
    "                'weight': self.init_params['feature_weights']['one_hot'],\n",
    "                'std_scale': 1.0,  # ✓ Binary features\n",
    "                'mean': 0.047      # ✓ From one-hot analysis\n",
    "            },\n",
    "            'pssm': {\n",
    "                'start': 21, 'end': 42,\n",
    "                'weight': self.init_params['feature_weights']['pssm'],\n",
    "                'std_scale': 1.0,  # ✓ Base scale\n",
    "                'mean': 0.136      # ✓ From PSSM analysis\n",
    "            },\n",
    "            'aux': {\n",
    "                'start': 42, 'end': 46,\n",
    "                'weight': self.init_params['feature_weights']['aux'],\n",
    "                'std_scale': 0.7,  # ✓ From property analysis\n",
    "                'mean': 0.268      # ✓ From property analysis\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Stability configuration (from stability analysis)\n",
    "        self.stability_config = {\n",
    "            'max_lr_scale': 2.0,                # ⚠️ Could be refined\n",
    "            'balance_threshold': 1.399835e-04,  # ✓ From stability analysis\n",
    "            'update_scale': 1.0                 # ✓ From update scale analysis\n",
    "        }\n",
    "\n",
    "        self.emission_config = {\n",
    "            'base_std': 0.089,\n",
    "            'init_noise_scale': 0.098,\n",
    "            'state_biases': {\n",
    "                'helix': [0.030, 0.325, 0.645],\n",
    "                'sheet': [0.059, 0.449, 0.492],\n",
    "                'coil': [0.172, 0.765, 0.063],\n",
    "            },\n",
    "            'update_clip': 0.021\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # Add new logging control parameters\n",
    "        self.logging_config = {\n",
    "            'state_collapse_window': 100,  # Number of positions to average over\n",
    "            'logging_frequency': 10,        # Log every N iterations\n",
    "            'min_warning_interval': 2.0    # Minimum seconds between collapse warnings\n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "## ProteinFeatures Class\n",
    "## Previously: Used mask from seq_data[:, -1] to find valid positions, which was incorrect as actual data was in first ~67 positions\n",
    "## Fixed: Now using one-hot encoding sums to find true sequence positions, ensuring we don't miss the actual data\n",
    "## Key Issue: Original code was looking at positions 67+ while actual data was in 0-66 range\n",
    "class ProteinFeatures:\n",
    "    def __init__(self, npy_data: np.ndarray):\n",
    "        \"\"\"Initialize with raw NPY data\"\"\"\n",
    "        self.raw_data = npy_data\n",
    "        self.current_idx = 0\n",
    "        logger.info(f\"Initializing feature extraction for sequence shape: {npy_data.shape}\")\n",
    "        \n",
    "    def extract_features(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Extract features with verified dimensions while preserving all feature information\"\"\"\n",
    "        if self.current_idx >= len(self.raw_data):\n",
    "            raise StopIteration(\"No more sequences to process\")\n",
    "            \n",
    "        # Get current sequence and reshape\n",
    "        seq_data = self.raw_data[self.current_idx].reshape(700, 57)\n",
    "        one_hot_sums = np.sum(seq_data[:, :21], axis=1)\n",
    "        valid_positions = np.where(one_hot_sums > 0)[0]\n",
    "        \n",
    "        if len(valid_positions) == 0:\n",
    "            logger.warning(f\"Empty sequence at index {self.current_idx}\")\n",
    "            self.current_idx += 1\n",
    "            raise ValueError(f\"Empty sequence at index {self.current_idx-1}\")\n",
    "        \n",
    "        seq_length = len(valid_positions)\n",
    "        if self.current_idx % 100 == 0 or self.current_idx == 1 or self.current_idx == len(self.raw_data) - 1:\n",
    "            logger.debug(f\"Processing sequence {self.current_idx}/{len(self.raw_data)}, length: {seq_length}\")\n",
    "        \n",
    "        self.current_idx += 1\n",
    "        \n",
    "        # Extract all feature components (preserving existing structure)\n",
    "        one_hot = seq_data[valid_positions, :21]                        # 21 features\n",
    "        pssm = seq_data[valid_positions, 21:42]                        # 21 features\n",
    "        ss8 = seq_data[valid_positions, 42:50]                         # Secondary structure (for labels)\n",
    "        disorder = seq_data[valid_positions, 50:51]                    # 1 feature\n",
    "        additional = seq_data[valid_positions, 51:]                    # Additional features\n",
    "        \n",
    "        # Create position features\n",
    "        rel_pos = np.arange(seq_length) / seq_length\n",
    "        start_dist = np.arange(seq_length) / seq_length\n",
    "        end_dist = np.arange(seq_length)[::-1] / seq_length\n",
    "        pos_features = np.stack([rel_pos, start_dist, end_dist], axis=1)\n",
    "        \n",
    "        # Store full feature information for debugging and analysis\n",
    "        self.feature_info = {\n",
    "            'one_hot': one_hot,\n",
    "            'pssm': pssm,\n",
    "            'ss8': ss8,\n",
    "            'disorder': disorder,\n",
    "            'additional': additional,\n",
    "            'positional': pos_features\n",
    "        }\n",
    "        \n",
    "        # Combine features with verified dimensions (21 + 21 + 1 + 3 = 46 total)\n",
    "        features = np.concatenate([one_hot, pssm, disorder, pos_features], axis=1)\n",
    "        \n",
    "        # Convert SS8 to SS3 for labels\n",
    "        ss3 = self._convert_dssp8_to_dssp3(ss8)\n",
    "        \n",
    "        # Validate feature dimensions while preserving debug info\n",
    "        if features.shape[1] != 46:\n",
    "            logger.error(f\"Feature dimension mismatch: {features.shape[1]} != 46\")\n",
    "            logger.debug(\"Feature shapes:\")\n",
    "            for name, feat in self.feature_info.items():\n",
    "                logger.debug(f\"  {name}: {feat.shape}\")\n",
    "            raise ValueError(f\"Feature dimension mismatch: {features.shape[1]} != 46\")\n",
    "            \n",
    "        return features, ss3\n",
    "\n",
    "    \n",
    "    def _convert_dssp8_to_dssp3(self, ss8: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert 8-state DSSP to 3-state representation using dominant states\"\"\"\n",
    "        dssp3 = np.zeros((len(ss8), 3))\n",
    "        dominant_states = np.argmax(ss8, axis=1)\n",
    "        \n",
    "        # Map states: H,G,I -> Helix; E,B -> Sheet; T,S,C -> Coil\n",
    "        dssp3[:, 0] = np.isin(dominant_states, [0,1,2])  # Helix\n",
    "        dssp3[:, 1] = np.isin(dominant_states, [3,4])    # Sheet\n",
    "        dssp3[:, 2] = np.isin(dominant_states, [5,6,7])  # Coil\n",
    "        \n",
    "        return dssp3\n",
    "    \n",
    "    def _create_position_features(self, length: int) -> np.ndarray:\n",
    "        \"\"\"Create position-specific features\"\"\"\n",
    "        # Relative position in sequence\n",
    "        rel_pos = np.arange(length) / length\n",
    "        # Distance from sequence ends\n",
    "        start_dist = np.arange(length) / length\n",
    "        end_dist = np.arange(length)[::-1] / length\n",
    "        return np.stack([rel_pos, start_dist, end_dist], axis=1)\n",
    "\n",
    "\n",
    "## -----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "class MixtureGaussianHMM:\n",
    "    \"\"\"HMM with mixture of Gaussians emission for protein structure prediction\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"Initialize model with given configuration\"\"\"\n",
    "        self.config = config\n",
    "        np.random.seed(config.random_seed)\n",
    "        self._initialize_model()\n",
    "        logger.info(f\"Initialized HMM with {config.n_states} states and {config.n_mixtures} mixtures per state\")\n",
    "\n",
    "    ## DO. NOT. USE. THE. MASK. its so broken!!! use this instead.\n",
    "    @staticmethod\n",
    "    def _get_valid_positions(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Centralized method for getting valid positions\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(-1, 57)\n",
    "        one_hot_sums = np.sum(x[:, :21], axis=1)\n",
    "        return np.where(one_hot_sums > 0)[0]\n",
    "\n",
    "    # Add Feature Validation Method to MixtureGaussianHMM\n",
    "    def _validate_feature_dimensions(self, x: np.ndarray) -> None:\n",
    "        \"\"\"Validate feature dimensions while preserving all error tracking\"\"\"\n",
    "        if x.shape[1] != self.config.n_features:\n",
    "            error_msg = f\"Feature dimension mismatch. Expected: {self.config.n_features}, Got: {x.shape[1]}\"\n",
    "            logger.error(error_msg)\n",
    "            logger.debug(f\"Feature shape details: {x.shape}\")\n",
    "            logger.debug(f\"Model feature config: {self.feature_config}\")\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "                        \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize model with validation\"\"\"\n",
    "        # Get initial stats from config\n",
    "        state_priors = np.array([self.config.init_params['state_priors'][s] for s in ['helix', 'sheet', 'coil']])\n",
    "        tp = self.config.init_params['transition_params']\n",
    "\n",
    "        # Calculate mixing with boundary check\n",
    "        mixing = max(0.05, min(tp['helix_to_sheet'], tp['sheet_to_coil'], tp['coil_to_helix']))\n",
    "\n",
    "        # Initialize transitions with guaranteed non-zero probabilities\n",
    "        self.transitions = np.array([\n",
    "            [tp['helix_self'], mixing, 1 - tp['helix_self'] - mixing],\n",
    "            [mixing, tp['sheet_self'], 1 - tp['sheet_self'] - mixing],\n",
    "            [mixing, 1 - tp['coil_self'] - mixing, tp['coil_self']]\n",
    "        ])\n",
    "            \n",
    "        # Ensure valid probabilities\n",
    "        self.transitions = np.maximum(self.transitions, 0.0)  # Ensure non-negative\n",
    "        row_sums = self.transitions.sum(axis=1, keepdims=True)\n",
    "        self.transitions = self.transitions / row_sums  # Normalize rows\n",
    "\n",
    "        # Initialize state priors\n",
    "        self.state_priors = state_priors / state_priors.sum()\n",
    "        \n",
    "        # Verify transition matrix before proceeding\n",
    "        self._check_transition_matrix()\n",
    "        # Initialize remaining components\n",
    "        self._initialize_mixture_weights()\n",
    "        self._initialize_emissions()\n",
    "\n",
    "\n",
    "    def _check_transition_matrix(self):\n",
    "        \"\"\"Validate transition matrix properties\"\"\"\n",
    "        # Check row sums\n",
    "        row_sums = np.sum(self.transitions, axis=1)\n",
    "        if not np.allclose(row_sums, 1.0, rtol=1e-5, atol=1e-5):\n",
    "            logger.error(f\"Invalid transition matrix:\\nRow sums: {row_sums}\\nMin value: {np.min(self.transitions)}\")\n",
    "            raise ValueError(\"Invalid transition matrix: row sums not 1 or negative values present\")\n",
    "        \n",
    "        # Check for and fix zero probabilities\n",
    "        min_prob = 1e-5\n",
    "        if np.any(self.transitions < min_prob):\n",
    "            logger.warning(f\"Found near-zero probabilities in transition matrix. Adjusting...\")\n",
    "            self.transitions = np.maximum(self.transitions, min_prob)\n",
    "            self.transitions /= self.transitions.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    \n",
    "    def _initialize_transitions(self):\n",
    "        \"\"\"Initialize transitions using config parameters\"\"\"\n",
    "        tp = self.config.init_params['transition_params']\n",
    "        \n",
    "        # Calculate mixing based on transition probabilities\n",
    "        ## old --> mixing = min(tp['helix_to_sheet'], tp['sheet_to_coil'], tp['coil_to_helix'])\n",
    "        mixing = max(0.05, min(tp['helix_to_sheet'], tp['sheet_to_coil'], tp['coil_to_helix']))  # Ensure minimum mixing\n",
    "\n",
    "        self.transitions = np.array([\n",
    "            [tp['helix_self'], mixing, 1 - tp['helix_self'] - mixing],\n",
    "            [mixing, tp['sheet_self'], 1 - tp['sheet_self'] - mixing],\n",
    "            [mixing, 1 - tp['coil_self'] - mixing, tp['coil_self']]\n",
    "        ])\n",
    "        \n",
    "        # Ensure minimum probabilities\n",
    "        self.transitions = np.maximum(self.transitions, 0.05)\n",
    "        self.transitions /= self.transitions.sum(axis=1, keepdims=True)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    def _initialize_emissions(self):\n",
    "        \"\"\"Initialize emissions using parameterized config\"\"\"\n",
    "        n_states, n_mix, n_feat = self.config.n_states, self.config.n_mixtures, self.config.n_features\n",
    "        ec = self.config.emission_config\n",
    "        ep = self.config.init_params['emission_params']\n",
    "        fc = self.config.feature_config\n",
    "            \n",
    "        # Initialize base parameters with smaller noise scale for stability\n",
    "        self.emission_means = np.random.normal(0, ec['init_noise_scale'] * 0.5,  # Reduced noise scale\n",
    "                                             size=(n_states, n_mix, n_feat))\n",
    "        self.emission_covs = np.ones((n_states, n_mix, n_feat)) * ec['base_std'] * 1.5  # Slightly increased base variance\n",
    "        \n",
    "        # Define boost factors from config\n",
    "        boost_factors = [ ep['helix_boost'], ep['sheet_boost'], ep['coil_boost'] ]\n",
    "\n",
    "        for state in range(n_states):\n",
    "            # Add gradual scaling across states\n",
    "            state_scale = 1.0 + (state - 1) * 0.1  # Small differential between states\n",
    "            \n",
    "            for feat_type, conf in fc.items():\n",
    "                start, end = conf['start'], conf['end']\n",
    "                # Balanced feature-specific scaling\n",
    "                self.emission_means[state, :, start:end] *= boost_factors[state] * state_scale\n",
    "                self.emission_covs[state, :, start:end] *= conf['std_scale'] * state_scale\n",
    "        \n",
    "        # Square covariances for positive definiteness\n",
    "        self.emission_covs = self.emission_covs ** 2\n",
    "        \n",
    "\n",
    "        \n",
    "    def _initialize_mixture_weights(self):\n",
    "        \"\"\"Initialize mixture weights using parameterized weights\"\"\"\n",
    "        # Use mixture weights from config instead of hardcoded values\n",
    "        base_weights = np.array([\n",
    "            self.config.init_params['mixture_weights']['primary'], self.config.init_params['mixture_weights']['secondary'],\n",
    "            self.config.init_params['mixture_weights']['tertiary'] ])\n",
    "        \n",
    "        # Create tile and apply minimum probability constraint\n",
    "        self.mixture_weights = np.tile(base_weights, (self.config.n_states, 1))\n",
    "        min_prob = self.config.min_mixture_prob\n",
    "        \n",
    "        # Apply constraints and normalize\n",
    "        self.mixture_weights = np.maximum(self.mixture_weights, min_prob)\n",
    "        self.mixture_weights /= self.mixture_weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Old version: Direct gradient updates with clipping\n",
    "    New version: Balance-weighted updates with feature-type specific handling\n",
    "    More sophisticated learning rate modulation based on state distribution\n",
    "    \n",
    "    -- Think of it like adding a correction term to keep the model from falling into state collapse, while preserving the underlying statistical learning mechanism.\n",
    "    -- added an overlay of state balance maintenance.\n",
    "    \"\"\"       \n",
    "    \n",
    "\n",
    "    ## State Balance Enforcement\n",
    "    ## Critical Issue: Was enforcing balance on possibly invalid state distributions\n",
    "    ## Fix: Now uses valid position counts and proper weighting\n",
    "    def _update_parameters(self, stats: Dict) -> None:\n",
    "        \"\"\"Parameter update coordinator with state balance enforcement and emission integration\"\"\"\n",
    "        # Get valid state usage with stability threshold\n",
    "        state_usage = stats['state_occurences'] \n",
    "        total_valid = np.sum(state_usage) + self.config.stability_config['balance_threshold']\n",
    "        state_dist = state_usage / total_valid\n",
    "        \n",
    "        # State balance analysis with deviation tracking\n",
    "        dist_deviation = self.config.target_state_dist - state_dist\n",
    "        max_scale = self.config.stability_config['max_lr_scale']\n",
    "        state_scales = np.clip(self.config.target_state_dist / (state_dist + 1e-10), 1.0, max_scale)\n",
    "        state_lr = self.config.learning_rate * state_scales\n",
    "        \n",
    "        # Analyze emission patterns and component usage\n",
    "        emission_stats = stats['emission_stats']\n",
    "        component_balance = emission_stats['component_usage'] / (np.sum(emission_stats['component_usage'], axis=1, keepdims=True) + 1e-10)\n",
    "        feature_importance = emission_stats['feature_contributions'] / (np.sum(emission_stats['feature_contributions'], axis=1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        # Update transitions with balance enforcement and component influence\n",
    "        self._update_transitions(\n",
    "            transition_counts=stats['transition_counts'],\n",
    "            state_dist=state_dist,\n",
    "            dist_deviation=dist_deviation,\n",
    "            component_usage=component_balance\n",
    "        )\n",
    "        \n",
    "        # Update emissions with state-specific learning rates and feature importance\n",
    "        self._update_emission_parameters(\n",
    "            emission_stats=emission_stats,\n",
    "            state_lr=state_lr,\n",
    "            feature_weights=feature_importance,\n",
    "            state_deviation=dist_deviation\n",
    "        )\n",
    "        \n",
    "        # Monitor parameter updates and stability\n",
    "        if np.any(dist_deviation > 0.2):\n",
    "            logger.warning(f\"Large state distribution deviation detected: {dist_deviation}\")\n",
    "            logger.debug(f\"Component balance: {component_balance}\")\n",
    "            logger.debug(f\"Feature importance: {feature_importance}\")\n",
    "    \n",
    "        \n",
    "    def _update_transitions(self, transition_counts: np.ndarray, state_dist: np.ndarray, dist_deviation: np.ndarray, component_usage: np.ndarray) -> None:\n",
    "        \"\"\"Update transition matrix with balance constraints and component influence\"\"\"\n",
    "        # Normalize counts with stability threshold\n",
    "        norm_counts = transition_counts / (transition_counts.sum(axis=1, keepdims=True) + self.config.stability_config['balance_threshold'])\n",
    "        \n",
    "        # Compute balance-adjusted updates with component influence\n",
    "        balance_factor = self.config.state_balance_weight\n",
    "        component_influence = np.mean(component_usage, axis=1)  # Average component usage per state\n",
    "        updates = norm_counts - self.transitions\n",
    "        \n",
    "        # Apply stronger updates for underrepresented states with component weighting\n",
    "        for i in range(self.config.n_states):\n",
    "            balance_update = balance_factor * dist_deviation\n",
    "            component_weight = np.clip(component_influence[i], 0.1, 0.9)  # Limit component influence\n",
    "            updates[i] += balance_update * component_weight\n",
    "        \n",
    "        # Apply updates with bounds and stability constraints\n",
    "        self.transitions = np.clip(\n",
    "            self.transitions + updates * self.config.stability_config['update_scale'],\n",
    "            self.config.min_state_prob,\n",
    "            self.config.max_state_prob\n",
    "        )\n",
    "        \n",
    "        # Renormalize while preserving minimum probability constraints\n",
    "        row_sums = self.transitions.sum(axis=1, keepdims=True)\n",
    "        self.transitions /= np.maximum(row_sums, 1e-10)\n",
    "        \n",
    "        # Monitor transition stability\n",
    "        if np.any(np.abs(updates) > 0.1):\n",
    "            logger.debug(f\"Large transition updates detected: max={np.max(np.abs(updates)):.3f}\")\n",
    "            logger.debug(f\"Component influence: {component_influence}\")\n",
    "\n",
    "            \n",
    "    def _update_emission_parameters(self, emission_stats: Dict, state_lr: np.ndarray, feature_weights: np.ndarray, state_deviation: np.ndarray) -> Dict:\n",
    "        \"\"\"Update emission parameters with state balance and feature importance integration\"\"\"\n",
    "        weights = emission_stats['weights_num']; means_num = emission_stats['means_num']; covs_num = emission_stats['covs_num']\n",
    "        param_changes = {'mean_shifts': np.zeros((self.config.n_states, self.config.n_mixtures)), 'cov_changes': np.zeros((self.config.n_states, self.config.n_mixtures))}\n",
    "        \n",
    "        for state in range(self.config.n_states):\n",
    "            for mix in range(self.config.n_mixtures):\n",
    "                if weights[state, mix] < 1e-10: continue\n",
    "                \n",
    "                # New means computation with feature weighting\n",
    "                new_means = means_num[state, mix] / (weights[state, mix] + 1e-10)\n",
    "                new_covs = covs_num[state, mix] / (weights[state, mix] + 1e-10)\n",
    "                \n",
    "                # Feature-type specific updates with learning rate adjustment\n",
    "                for feat_type, conf in self.config.feature_config.items():\n",
    "                    start, end = conf['start'], conf['end']; feat_slice = slice(start, end)\n",
    "                    feat_lr = state_lr[state] * conf['weight'] * feature_weights[state, conf['end'] - conf['start']]\n",
    "                    \n",
    "                    old_means = self.emission_means[state, mix, feat_slice].copy()\n",
    "                    self.emission_means[state, mix, feat_slice] = (1 - feat_lr) * old_means + feat_lr * new_means[feat_slice]\n",
    "                    \n",
    "                    # Update covariances with bounds and stability constraints\n",
    "                    new_covs_bounded = np.clip(new_covs[feat_slice], self.config.stability_config['balance_threshold'], self.config.emission_config['base_std'] * 4)\n",
    "                    self.emission_covs[state, mix, feat_slice] = (1 - feat_lr) * self.emission_covs[state, mix, feat_slice] + feat_lr * new_covs_bounded\n",
    "                    \n",
    "                    param_changes['mean_shifts'][state, mix] += np.mean(np.abs(self.emission_means[state, mix, feat_slice] - old_means))\n",
    "                \n",
    "                # Ensure minimum variance with state balance influence\n",
    "                min_var = self.config.stability_config['balance_threshold'] * (1 + np.abs(state_deviation[state]))\n",
    "                self.emission_covs[state, mix] = np.maximum(self.emission_covs[state, mix], min_var)\n",
    "        \n",
    "        if np.any(param_changes['mean_shifts'] > 0.1): logger.debug(f\"Large emission shifts detected: {param_changes['mean_shifts']}\")\n",
    "        return param_changes\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    ## MixtureGaussianHMM Core Methods\n",
    "    ## Previous Issue: Emission likelihood computation was using wrong positions\n",
    "    ## Fixed: Now properly handling sequence positions and structure state extraction\n",
    "    def _compute_emission_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute emission likelihood with corrected feature handling and stability monitoring\"\"\"\n",
    "        # Initialize output array for full sequence\n",
    "        seq_len = len(x)\n",
    "        emission_ll = np.zeros((seq_len, self.config.n_states))\n",
    "        state_stats = {state: {'max_ll': -np.inf, 'min_ll': np.inf} for state in range(self.config.n_states)}\n",
    "    \n",
    "        # Get valid positions and prepare data\n",
    "        valid_pos = self._get_valid_positions(x)\n",
    "        x_valid = x[valid_pos]\n",
    "        \n",
    "        # Define feature ranges based on config - ensures consistent slicing\n",
    "        feature_ranges = {'one_hot': slice(0, 21), 'pssm': slice(21, 42), 'aux': slice(42, 46)}  # one-hot: 21, PSSM: 21, aux: 4 features\n",
    "        \n",
    "        # Process each state\n",
    "        for state in range(self.config.n_states):\n",
    "            mixture_ll = np.zeros((len(valid_pos), self.config.n_mixtures))  # Setup mixture likelihoods array\n",
    "           \n",
    "            # Process each mixture component\n",
    "            for mix in range(self.config.n_mixtures):\n",
    "                group_ll = np.zeros(len(valid_pos))  # Initialize group log probabilities\n",
    "               \n",
    "                # Process each feature group (one-hot, PSSM, auxiliary)\n",
    "                for feat_name, feat_slice in feature_ranges.items():\n",
    "                    feat_ll = self._compute_feature_likelihood(x_valid[:, feat_slice], self.emission_means[state, mix, feat_slice], self.emission_covs[state, mix, feat_slice], self.config.feature_config[feat_name]['weight'])  # Compute feature likelihood with proper weight\n",
    "                    group_ll += feat_ll\n",
    "               \n",
    "                mixture_ll[:, mix] = group_ll + np.log(self.mixture_weights[state, mix] + self.config.stability_config['balance_threshold'])  # Add mixture weight and store\n",
    "           \n",
    "            max_ll = mixture_ll.max(axis=1, keepdims=True)  # Compute state emission likelihood with numerical stability (log-sum-exp trick)\n",
    "            emission_ll[valid_pos, state] = max_ll.squeeze() + np.log(np.sum(np.exp(mixture_ll - max_ll), axis=1))\n",
    "            state_stats[state].update({'max_ll': emission_ll[valid_pos, state].max(), 'min_ll': emission_ll[valid_pos, state].min(), 'mean_ll': emission_ll[valid_pos, state].mean()})  # Update state statistics\n",
    "        \n",
    "        return self._normalize_probabilities(emission_ll)  # Return normalized probabilities\n",
    "        \n",
    "    \n",
    "        \n",
    "    def _compute_feature_likelihood(self, x: np.ndarray, mean: np.ndarray, \n",
    "                                  cov: np.ndarray, weight: float) -> np.ndarray:\n",
    "        \"\"\"Compute weighted feature likelihood with numerical stability\"\"\"\n",
    "        # Input validation\n",
    "        if x.ndim != 2:   x = np.atleast_2d(x)\n",
    "        \n",
    "        n_samples, n_features = x.shape\n",
    "        if mean.size != n_features or cov.size != n_features:\n",
    "            raise ValueError(f\"Shape mismatch - x: {x.shape}, mean: {mean.shape}, \"\n",
    "                            f\"cov: {cov.shape}, expected features: {n_features}\")\n",
    "        \n",
    "        # Reshape mean and cov for broadcasting\n",
    "        mean = np.atleast_2d(mean);        cov = np.atleast_2d(cov);\n",
    "        \n",
    "        if mean.shape[1] != n_features:            mean = mean.T\n",
    "        if cov.shape[1] != n_features:            cov = cov.T\n",
    "            \n",
    "        # Ensure minimum variance for stability\n",
    "        stable_cov = np.maximum(cov, self.config.stability_config['balance_threshold'])\n",
    "        \n",
    "        # Compute likelihood\n",
    "        diff = x - mean\n",
    "        exp_term = -0.5 * np.sum((diff ** 2) / stable_cov, axis=1)\n",
    "        log_norm = -0.5 * np.sum(np.log(2 * np.pi * stable_cov))\n",
    "        \n",
    "        return weight * (exp_term + log_norm)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def _normalize_probabilities(self, log_probs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize log probabilities with reduced warning frequency\"\"\"\n",
    "        max_lp = log_probs.max(axis=1, keepdims=True)\n",
    "        probs = np.exp(log_probs - max_lp)\n",
    "        normalized = probs / (probs.sum(axis=1, keepdims=True) + self.config.stability_config['balance_threshold'])\n",
    "        \n",
    "        # Compute average state probabilities over window\n",
    "        window_size = min(self.config.logging_config['state_collapse_window'], normalized.shape[0])\n",
    "        state_probs = normalized[:window_size].mean(axis=0)\n",
    "        # Check state collapse only periodically using averaged probabilities\n",
    "        curr_time = time.time()\n",
    "        if not hasattr(self, '_last_warning_time'):\n",
    "            self._last_warning_time = 0\n",
    "\n",
    "        # Only log warning if sufficient time has passed and probability is below threshold\n",
    "        if (min(state_probs) < self.config.min_state_prob and \n",
    "            curr_time - self._last_warning_time >= self.config.logging_config['min_warning_interval']):\n",
    "            logger.warning(f\"State probability collapse detected:\")\n",
    "            logger.warning(f\"State distributions: {[f'{p:.6f}' for p in state_probs]}\")\n",
    "            logger.warning(f\"Min allowed prob: {self.config.min_state_prob}\")\n",
    "            logger.warning(f\"Max allowed prob: {self.config.max_state_prob}\")\n",
    "            self._last_warning_time = curr_time\n",
    "                \n",
    "        return normalized\n",
    "\n",
    "\n",
    "\n",
    "    def monitor_training_iteration(self, iteration: int, stats: Dict) -> None:\n",
    "        \"\"\"Monitor critical training statistics for debugging\"\"\"\n",
    "        if iteration % 5 == 0:  # Log every 5 iterations\n",
    "            state_usage = stats['state_occurences'] / stats['state_occurences'].sum()\n",
    "            transition_diag = np.diag(self.transitions)\n",
    "            \n",
    "            logger.info(f\"\\nIteration {iteration} Statistics:\")\n",
    "            logger.info(f\"State Distribution: {[f'{p:.3f}' for p in state_usage]}\")\n",
    "            logger.info(f\"Self-Transition Probs: {[f'{p:.3f}' for p in transition_diag]}\")\n",
    "            \n",
    "            # Monitor emission parameter spread\n",
    "            for state in range(self.config.n_states):\n",
    "                mean_range = (self.emission_means[state].min(), \n",
    "                             self.emission_means[state].max())\n",
    "                logger.info(f\"State {state} Emission Range: [{mean_range[0]:.3f}, \"\n",
    "                           f\"{mean_range[1]:.3f}]\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def _compute_gaussian_ll(self, x: np.ndarray, mean: np.ndarray, cov: np.ndarray, weight: float = 1.0) -> np.ndarray:\n",
    "        \"\"\"Compute weighted Gaussian log-likelihood with numerical stability\"\"\"\n",
    "        cov_stable = np.clip(cov, self.config.min_std, self.config.max_std)\n",
    "        diff = x - mean\n",
    "        exp_term = -0.5 * np.sum((diff ** 2) / cov_stable, axis=1)\n",
    "        log_norm = -0.5 * np.sum(np.log(2 * np.pi * cov_stable))\n",
    "        return weight * (exp_term + log_norm)\n",
    "\n",
    "    \n",
    "    \n",
    "    ## Mixture Component Responsibilities\n",
    "    ## Critical Issue: Not handling feature ranges correctly in probability computation\n",
    "    ## Fix: Now properly computes probabilities for valid features only\n",
    "    def _compute_mixture_responsibilities(self, x: np.ndarray, state: int, posteriors: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Compute responsibilities with enhanced component analysis\"\"\"\n",
    "        \n",
    "        valid_pos = self._get_valid_positions(x)\n",
    "        x_valid = x[valid_pos]\n",
    "        seq_len = len(x); \n",
    "        resp = np.zeros((seq_len, self.config.n_mixtures)); \n",
    "        mixture_stats = {'component_weights': np.zeros(self.config.n_mixtures), \n",
    "                         'feature_contributions': {feat: np.zeros(self.config.n_mixtures) for feat in self.config.feature_config}, \n",
    "                         'dominance_patterns': np.zeros(self.config.n_mixtures), 'confidence_metrics': []}\n",
    "        \n",
    "        # Handle each mixture component with feature-specific tracking\n",
    "        for mix in range(self.config.n_mixtures):\n",
    "            log_probs = np.zeros(seq_len); mixture_feat_contribs = np.zeros(len(self.config.feature_config)); start_idx = 0\n",
    "            \n",
    "            # Process each feature group with contribution tracking\n",
    "            for feat_idx, (feat_type, conf) in enumerate(self.config.feature_config.items()):\n",
    "                feat_len = conf['end'] - conf['start']; feat_slice = slice(start_idx, start_idx + feat_len)\n",
    "                feat_contribution = self._compute_feature_likelihood(x[:, feat_slice], self.emission_means[state, mix, feat_slice], self.emission_covs[state, mix, feat_slice], conf['weight'])\n",
    "                log_probs += feat_contribution; mixture_feat_contribs[feat_idx] = np.mean(np.abs(feat_contribution)); start_idx += feat_len\n",
    "            \n",
    "            # Add mixture weight and track statistics\n",
    "            log_probs += np.log(self.mixture_weights[state, mix] + 1e-10); resp[:, mix] = np.exp(log_probs)\n",
    "            mixture_stats['component_weights'][mix] = np.mean(resp[:, mix])\n",
    "            for feat_idx, feat_type in enumerate(self.config.feature_config):\n",
    "                mixture_stats['feature_contributions'][feat_type][mix] = mixture_feat_contribs[feat_idx]\n",
    "        \n",
    "        # Normalize responsibilities and track statistics\n",
    "        normalizer = resp.sum(axis=1, keepdims=True); resp /= np.maximum(normalizer, 1e-10)\n",
    "        mixture_stats['dominance_patterns'] = np.mean(resp, axis=0); mixture_stats['confidence_metrics'] = np.max(resp, axis=1)\n",
    "        \n",
    "        # Weight by state posteriors and log significant patterns\n",
    "        resp *= posteriors.reshape(-1, 1)\n",
    "        if np.max(mixture_stats['dominance_patterns']) > 0.8:\n",
    "            logger.debug(f\"Strong component dominance in state {state}: {mixture_stats['dominance_patterns']}\")\n",
    "        \n",
    "        return resp, mixture_stats    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Training Flow:\n",
    "    1. forward() and backward() - Core probability computation\n",
    "    2. _compute_mixture_responsibilities() - Component analysis\n",
    "    3. _collect_statistics() - Aggregates data from 1 & 2\n",
    "    4. _process_training_iteration() - Uses 3's output\n",
    "    5. _update_parameters() - Uses 4's output\n",
    "    6. train() - Orchestrates 4 & 5\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ## Forward Algorithm\n",
    "    ## Critical Issue: Was processing all positions including invalid ones\n",
    "    ## Fix: Now properly handles only valid sequence positions\n",
    "    def forward(self, x: np.ndarray) -> Tuple[np.ndarray, float, Dict]:\n",
    "        \"\"\"Forward algorithm with correct position handling and enhanced monitoring\"\"\"\n",
    "        # Find valid positions using one-hot encoding\n",
    "        one_hot = x[:, :21]\n",
    "        valid_pos = np.where(np.sum(one_hot, axis=1) > 0)[0]\n",
    "        seq_len = len(valid_pos)\n",
    "        \n",
    "        alpha = np.zeros((seq_len, self.config.n_states))\n",
    "        scaling = np.zeros(seq_len)\n",
    "        \n",
    "        # Track state probabilities with enhanced monitoring\n",
    "        state_probs = {\n",
    "            'max_prob': np.zeros(self.config.n_states),\n",
    "            'min_prob': np.ones(self.config.n_states),\n",
    "            'mean_prob': np.zeros(self.config.n_states),\n",
    "            'std_prob': [],\n",
    "            'stability_metrics': [],  # Added this key\n",
    "            'detailed': {\n",
    "                'position_distributions': [],\n",
    "                'scaling_factors': [],\n",
    "                'stability_metrics': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Get emission likelihoods for valid positions only\n",
    "        ## old: changed:: emission_probs = self._compute_emission_likelihood(x[valid_pos])\n",
    "        emission_probs = self._compute_emission_likelihood(x) \n",
    "\n",
    "        # Initialize forward variables with monitoring\n",
    "        alpha[0] = self.state_priors * emission_probs[0]\n",
    "        scaling[0] = np.sum(alpha[0]) + 1e-10\n",
    "        alpha[0] /= scaling[0]\n",
    "        \n",
    "        # Track initial state probabilities\n",
    "        for state in range(self.config.n_states):\n",
    "            state_probs['max_prob'][state] = alpha[0, state]\n",
    "            state_probs['min_prob'][state] = alpha[0, state]\n",
    "            state_probs['mean_prob'][state] = alpha[0, state]\n",
    "        state_probs['detailed']['position_distributions'].append(alpha[0].copy())\n",
    "        state_probs['detailed']['scaling_factors'].append(scaling[0])\n",
    "        \n",
    "        # Forward recursion with enhanced monitoring\n",
    "        for t in range(1, seq_len):\n",
    "            for j in range(self.config.n_states):\n",
    "                alpha[t, j] = np.sum(alpha[t-1] * self.transitions[:, j]) * emission_probs[t, j]\n",
    "                \n",
    "                # Update state probability tracking\n",
    "                state_probs['max_prob'][j] = max(state_probs['max_prob'][j], alpha[t, j])\n",
    "                state_probs['min_prob'][j] = min(state_probs['min_prob'][j], alpha[t, j])\n",
    "                state_probs['mean_prob'][j] += alpha[t, j]\n",
    "            \n",
    "            scaling[t] = np.sum(alpha[t]) + 1e-10\n",
    "            alpha[t] /= scaling[t]\n",
    "            \n",
    "            # Track distributions and stability\n",
    "            state_probs['detailed']['position_distributions'].append(alpha[t].copy())\n",
    "            state_probs['detailed']['scaling_factors'].append(scaling[t])\n",
    "            state_probs['std_prob'].append(np.std(alpha[t]))\n",
    "            \n",
    "            # Monitor stability\n",
    "            stability_metric = np.mean(np.abs(alpha[t]))\n",
    "            state_probs['detailed']['stability_metrics'].append(stability_metric)\n",
    "            \n",
    "        # Finalize mean probabilities\n",
    "        state_probs['mean_prob'] /= seq_len\n",
    "        log_likelihood = np.sum(np.log(scaling))\n",
    "        \n",
    "        # Monitor for numerical stability issues\n",
    "        if np.any(np.isnan(alpha)) or np.any(np.isinf(alpha)):\n",
    "            logger.warning(\"Numerical stability issues detected in forward algorithm\")\n",
    "            logger.debug(f\"State probability ranges: {state_probs}\")\n",
    "\n",
    "\n",
    "        \n",
    "        return alpha, log_likelihood, state_probs\n",
    "\n",
    "\n",
    "    \n",
    "    ## Backward Algorithm \n",
    "    ## Critical Issue: Was using scaling values for all positions, mismatched with forward pass\n",
    "    ## Fix: Now properly aligned with valid positions from forward pass\n",
    "    def backward(self, x: np.ndarray, scaling: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Backward algorithm with proper position handling and stability monitoring\"\"\"\n",
    "        # Get valid positions\n",
    "        one_hot = x[:, :21]\n",
    "        valid_pos = np.where(np.sum(one_hot, axis=1) > 0)[0]\n",
    "        seq_len = len(valid_pos)\n",
    "        \n",
    "        beta = np.zeros((seq_len, self.config.n_states))\n",
    "        emission_probs = self._compute_emission_likelihood(x[valid_pos])\n",
    "        \n",
    "        # Enhanced backward statistics tracking\n",
    "        backward_stats = {\n",
    "            'max_beta': np.zeros(self.config.n_states),\n",
    "            'min_beta': np.ones(self.config.n_states),\n",
    "            'mean_beta': np.zeros(self.config.n_states),\n",
    "            'stability_metrics': [],\n",
    "            'detailed': {\n",
    "                'position_distributions': [],\n",
    "                'transition_influence': np.zeros((self.config.n_states, self.config.n_states)),\n",
    "                'scaling_impact': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize backward variables\n",
    "        beta[-1] = 1.0 / scaling[-1]\n",
    "        backward_stats['detailed']['position_distributions'].append(beta[-1].copy())\n",
    "        \n",
    "        # Track initial state\n",
    "        for state in range(self.config.n_states):\n",
    "            backward_stats['max_beta'][state] = beta[-1, state]\n",
    "            backward_stats['min_beta'][state] = beta[-1, state]\n",
    "            backward_stats['mean_beta'][state] = beta[-1, state]\n",
    "        \n",
    "        # Backward recursion with enhanced stability monitoring\n",
    "        for t in range(seq_len-2, -1, -1):\n",
    "            for i in range(self.config.n_states):\n",
    "                # Compute transitions with emission probabilities\n",
    "                trans_probs = self.transitions[i] * emission_probs[t+1]\n",
    "                beta_contrib = trans_probs * beta[t+1]\n",
    "                beta[t, i] = np.sum(beta_contrib)\n",
    "                \n",
    "                # Track transition influence\n",
    "                backward_stats['detailed']['transition_influence'][i] += beta_contrib / (np.sum(beta_contrib) + 1e-10)\n",
    "                \n",
    "                # Track state probabilities\n",
    "                backward_stats['max_beta'][i] = max(backward_stats['max_beta'][i], beta[t, i])\n",
    "                backward_stats['min_beta'][i] = min(backward_stats['min_beta'][i], beta[t, i])\n",
    "                backward_stats['mean_beta'][i] += beta[t, i]\n",
    "            \n",
    "            # Scale and check stability\n",
    "            scaling_factor = scaling[t]\n",
    "            beta[t] /= scaling_factor\n",
    "            backward_stats['detailed']['scaling_impact'].append(scaling_factor)\n",
    "            \n",
    "            # Monitor stability\n",
    "            stability_metric = np.mean(np.abs(beta[t]))\n",
    "            backward_stats['stability_metrics'].append(stability_metric)\n",
    "            \n",
    "            # Track position distribution\n",
    "            backward_stats['detailed']['position_distributions'].append(beta[t].copy())\n",
    "            \n",
    "            if stability_metric > 1e3 or stability_metric < 1e-3:\n",
    "                logger.debug(f\"Potential stability issue at position {t}: metric = {stability_metric}\")\n",
    "        \n",
    "        # Finalize statistics\n",
    "        backward_stats['mean_beta'] /= seq_len\n",
    "        \n",
    "        # Check for numerical issues\n",
    "        if np.any(np.isnan(beta)) or np.any(np.isinf(beta)):\n",
    "            logger.warning(\"Numerical instability detected in backward algorithm\")\n",
    "            logger.debug(f\"Stability metrics: mean={np.mean(backward_stats['stability_metrics'])}, std={np.std(backward_stats['stability_metrics'])}\")\n",
    "        \n",
    "        return beta, backward_stats    \n",
    "\n",
    "\n",
    "\n",
    "    ## Viterbi Algorithm\n",
    "    ## Critical Issue: Was finding paths through invalid positions\n",
    "    ## Fix: Now only considers valid sequence positions\n",
    "    def viterbi(self, x: np.ndarray) -> Tuple[List[int], float, Dict]:\n",
    "        \"\"\"Viterbi algorithm with proper position handling and enhanced path analysis\"\"\"\n",
    "        # Get valid positions\n",
    "        one_hot = x[:, :21]; valid_pos = np.where(np.sum(one_hot, axis=1) > 0)[0]; seq_len = len(valid_pos)\n",
    "        \n",
    "        # Initialize tracking structures\n",
    "        viterbi_vars = np.zeros((seq_len, self.config.n_states)); backpointers = np.zeros((seq_len, self.config.n_states), dtype=np.int32)\n",
    "        path_stats = {'state_transitions': np.zeros((self.config.n_states, self.config.n_states)), 'state_confidences': [], 'path_probabilities': []}\n",
    "        \n",
    "        # Get emission probabilities with monitoring\n",
    "        emission_probs = self._compute_emission_likelihood(x[valid_pos])\n",
    "        \n",
    "        # Initialize with state priors and track confidence\n",
    "        viterbi_vars[0] = np.log(self.state_priors + 1e-10) + np.log(emission_probs[0] + 1e-10)\n",
    "        path_stats['state_confidences'].append(np.exp(viterbi_vars[0] - np.max(viterbi_vars[0])))\n",
    "        \n",
    "        # Recursion with enhanced monitoring\n",
    "        for t in range(1, seq_len):\n",
    "            for j in range(self.config.n_states):\n",
    "                trans_probs = viterbi_vars[t-1] + np.log(self.transitions[:, j] + 1e-10)\n",
    "                backpointers[t, j] = np.argmax(trans_probs)\n",
    "                viterbi_vars[t, j] = trans_probs[backpointers[t, j]] + np.log(emission_probs[t, j] + 1e-10)\n",
    "            \n",
    "            # Track state confidences and probabilities\n",
    "            position_probs = np.exp(viterbi_vars[t] - np.max(viterbi_vars[t]))\n",
    "            path_stats['state_confidences'].append(position_probs)\n",
    "            path_stats['path_probabilities'].append(np.max(position_probs))\n",
    "        \n",
    "        # Backtrack with transition analysis\n",
    "        path = [0] * seq_len; path[-1] = np.argmax(viterbi_vars[-1]); path_score = viterbi_vars[-1, path[-1]]\n",
    "        \n",
    "        for t in range(seq_len-2, -1, -1):\n",
    "            path[t] = backpointers[t+1, path[t+1]]\n",
    "            # Track state transitions in path\n",
    "            if t < seq_len-1:\n",
    "                path_stats['state_transitions'][path[t], path[t+1]] += 1\n",
    "        \n",
    "        # Compute additional path statistics\n",
    "        path_stats.update({\n",
    "            'mean_confidence': np.mean(path_stats['path_probabilities']),\n",
    "            'min_confidence': np.min(path_stats['path_probabilities']),\n",
    "            'state_frequencies': np.bincount(path, minlength=self.config.n_states) / len(path),\n",
    "            'transition_frequencies': path_stats['state_transitions'] / max(1, np.sum(path_stats['state_transitions']))\n",
    "        })\n",
    "        \n",
    "        # Log interesting patterns\n",
    "        if path_stats['mean_confidence'] < 0.5:\n",
    "            logger.warning(f\"Low confidence path detected: mean={path_stats['mean_confidence']:.3f}\")\n",
    "        \n",
    "        return path, path_score, path_stats\n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "    def compute_posteriors(self, alpha: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute state posteriors from forward-backward variables with enhanced stability\"\"\"\n",
    "        # Compute raw posteriors with numerical stability handling\n",
    "        posteriors = alpha * beta; scaling_factors = np.maximum(posteriors.sum(axis=1, keepdims=True), self.config.stability_config['balance_threshold'])\n",
    "        \n",
    "        # Normalize with stability threshold and state balance influence\n",
    "        normalized_posteriors = posteriors / scaling_factors\n",
    "        \n",
    "        # Track extreme probability events for monitoring\n",
    "        if np.any(normalized_posteriors > 0.99) or np.any(normalized_posteriors < 0.01):\n",
    "            state_dist = normalized_posteriors.mean(axis=0)\n",
    "            logger.debug(f\"Extreme posterior probabilities detected: min={normalized_posteriors.min():.3f}, max={normalized_posteriors.max():.3f}\")\n",
    "            logger.debug(f\"State distribution: {[f'{p:.3f}' for p in state_dist]}\")\n",
    "        \n",
    "        return normalized_posteriors\n",
    "\n",
    "\n",
    "    ## ----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def _debug_state_statistics(self, x: np.ndarray, state: int) -> Dict:\n",
    "        \"\"\"Analyze state-specific statistics for debugging\"\"\"\n",
    "        # Get emission probabilities\n",
    "        emission_probs = self._compute_emission_likelihood(x)\n",
    "        \n",
    "        # Analyze state probabilities\n",
    "        state_stats = { 'max_prob': emission_probs[:, state].max(), 'min_prob': emission_probs[:, state].min(), 'mean_prob': emission_probs[:, state].mean(), \n",
    "            'std_prob': emission_probs[:, state].std() }\n",
    "        \n",
    "        # Analyze transitions\n",
    "        state_stats.update({ 'incoming_trans': self.transitions[:, state].copy(), \n",
    "                            'outgoing_trans': self.transitions[state, :].copy(), 'self_trans': self.transitions[state, state] })\n",
    "        \n",
    "        # Analyze emissions\n",
    "        for mix in range(self.config.n_mixtures):\n",
    "            mean_stats = self.emission_means[state, mix]\n",
    "            cov_stats = self.emission_covs[state, mix]\n",
    "            state_stats.update({ f'mix_{mix}_mean_range': (mean_stats.min(), mean_stats.max()), f'mix_{mix}_cov_range': (cov_stats.min(), cov_stats.max()) })\n",
    "        \n",
    "        return state_stats\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def _validate_sequence(self, seq: np.ndarray, seq_idx: int) -> Optional[np.ndarray]:\n",
    "        \"\"\"Helper to validate and extract valid positions from a sequence\"\"\"\n",
    "        one_hot = seq[:, :21]; valid_pos = np.where(np.sum(one_hot, axis=1) > 0)[0]\n",
    "        return seq[valid_pos] if len(valid_pos) > 0 else None\n",
    "    \n",
    "    def _validate_sequences(self, sequences: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        \"\"\"Process all sequences and return valid ones\"\"\"\n",
    "        valid_seqs = [seq for idx, seq in enumerate(sequences) if (validated := self._validate_sequence(seq, idx)) is not None]\n",
    "        logger.info(f\"Found {len(valid_seqs)} valid sequences out of {len(sequences)}\")\n",
    "        if not valid_seqs: raise ValueError(\"No valid sequences found for training\")\n",
    "        return valid_seqs\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self, sequences: List[np.ndarray], val_sequences: Optional[List[np.ndarray]] = None, val_labels: Optional[List[np.ndarray]] = None, n_iterations: int = 100, tolerance: float = 1e-4) -> Dict[str, List[float]]:\n",
    "        \"\"\"Training with proper sequence validation and comprehensive monitoring\"\"\"\n",
    "        logger.info(f\"Starting training with {len(sequences)} sequences\"); \n",
    "        valid_sequences = self._validate_sequences(sequences)\n",
    "\n",
    "        # Add diagnostic prints right after model initialization\n",
    "        logger.info(\"\\nInitial Model State:\"); logger.info(f\"Transition probabilities:\\n{self.transitions}\"); logger.info(f\"Mixture weights:\\n{self.mixture_weights}\");\n",
    "        logger.info(f\"State priors:\\n{self.state_priors}\");\n",
    "        \n",
    "        # Monitor first sequence processing\n",
    "        first_seq = sequences[0]\n",
    "        valid_pos = self._get_valid_positions(first_seq);    logger.info(f\"\\nFirst sequence details:\");    logger.info(f\"Valid positions: {len(valid_pos)}\")    \n",
    "        # Check emission probabilities for first sequence\n",
    "        emission_probs = self._compute_emission_likelihood(first_seq[valid_pos]);  logger.info(f\"Initial emission probability ranges:\");\n",
    "        logger.info(f\"Min: {emission_probs.min():.6f}, Max: {emission_probs.max():.6f}\"); logger.info(f\"Mean per state: {emission_probs.mean(axis=0)}\");\n",
    "    \n",
    "        \n",
    "        # Validate val sequences if provided\n",
    "        valid_val = None if val_sequences is None else self._validate_sequences(val_sequences)\n",
    "        self.history = self._initialize_history(valid_val is not None)\n",
    "        \n",
    "        # Training state tracking\n",
    "        best_ll, best_params, no_improve, plateau_count = float('-inf'), None, 0, 0\n",
    "        self._collapse_counter = 0; self._last_warning_time = time.time()\n",
    "        \n",
    "        # Enhanced training monitoring\n",
    "        self.training_stats = {\n",
    "            'component_evolution': [],  # Track mixture component changes\n",
    "            'feature_importance': [],   # Track feature contributions\n",
    "            'state_dynamics': [],       # Track state distribution changes\n",
    "            'emission_variations': [],  # Track emission parameter changes\n",
    "            'convergence_metrics': []   # Track detailed convergence metrics\n",
    "        }\n",
    "        \n",
    "        # Main training loop with enhanced monitoring\n",
    "        for iteration in range(n_iterations):\n",
    "            # Process iteration and collect comprehensive stats\n",
    "            iter_stats = self._process_training_iteration(iteration, valid_sequences, self.history, valid_val, val_labels)\n",
    "            \n",
    "            # Update enhanced monitoring\n",
    "            self._update_training_stats(iter_stats)\n",
    "            \n",
    "            # Check convergence with modified criteria\n",
    "            should_stop, best_ll, best_params = self._check_convergence(iter_stats, best_ll, best_params, no_improve, plateau_count, tolerance)\n",
    "            \n",
    "            # Early stopping with state distribution validation\n",
    "            if should_stop and iteration >= 5:\n",
    "                if best_params: \n",
    "                    logger.info(\"Restoring best parameters before stopping\")\n",
    "                    self._restore_parameters(best_params)\n",
    "                break\n",
    "            \n",
    "            # Update parameters with monitoring\n",
    "            param_changes = self._update_parameters(iter_stats['train_stats'])\n",
    "            self.training_stats['emission_variations'].append(param_changes)\n",
    "            \n",
    "            # Learning rate adaptation\n",
    "            self._adjust_learning_rate(iter_stats)\n",
    "            \n",
    "            # Periodic detailed analysis\n",
    "            if iteration % self.config.logging_config['logging_frequency'] == 0:\n",
    "                self._perform_detailed_analysis(iter_stats)\n",
    "        \n",
    "        # Final analysis and logging\n",
    "        self._log_final_training_results(iter_stats, iteration)\n",
    "        return self.history, self.training_stats\n",
    "\n",
    "\n",
    "        \n",
    "    def _update_training_stats(self, iter_stats: Dict) -> None:\n",
    "        \"\"\"Update comprehensive training statistics\"\"\"\n",
    "        # Component evolution tracking\n",
    "        self.training_stats['component_evolution'].append({\n",
    "            'weights': [comp['component_weights'] for comp in iter_stats['mixture_stats']],\n",
    "            'dominance': [comp['dominance_patterns'] for comp in iter_stats['mixture_stats']]\n",
    "        })\n",
    "        \n",
    "        # Feature importance tracking\n",
    "        self.training_stats['feature_importance'].append({\n",
    "            'feature_contribs': iter_stats['feature_contributions'],\n",
    "            'importance_scores': iter_stats['feature_importance']\n",
    "        })\n",
    "        \n",
    "        # State dynamics\n",
    "        self.training_stats['state_dynamics'].append({\n",
    "            'distribution': iter_stats['state_dist'],\n",
    "            'transition_patterns': iter_stats['transition_patterns'],\n",
    "            'confidence': iter_stats['state_confidence']\n",
    "        })\n",
    "        \n",
    "        # Convergence metrics\n",
    "        self.training_stats['convergence_metrics'].append({\n",
    "            'likelihood_change': iter_stats['ll_change'],\n",
    "            'param_stability': iter_stats['param_stability'],\n",
    "            'state_balance': iter_stats['state_balance']\n",
    "        })\n",
    "    \n",
    "    def _perform_detailed_analysis(self, iter_stats: Dict) -> None:\n",
    "        \"\"\"Perform comprehensive analysis during training\"\"\"\n",
    "        logger.info(\"\\nDetailed Training Analysis:\")\n",
    "        \n",
    "        # Analyze state dynamics\n",
    "        state_dist = iter_stats['state_dist']\n",
    "        logger.info(\"\\nState Distribution Analysis:\")\n",
    "        for i, state in enumerate(['Helix', 'Sheet', 'Coil']):\n",
    "            logger.info(f\"{state}: {state_dist[i]:.3f}\")\n",
    "        \n",
    "        # Analyze mixture components\n",
    "        logger.info(\"\\nMixture Component Analysis:\")\n",
    "        for state in range(self.config.n_states):\n",
    "            comps = iter_stats['mixture_stats'][state]['component_weights']\n",
    "            logger.info(f\"State {state} components: {[f'{w:.3f}' for w in comps]}\")\n",
    "        \n",
    "        # Analyze feature contributions\n",
    "        logger.info(\"\\nFeature Contribution Analysis:\")\n",
    "        for feat, score in iter_stats['feature_importance'].items():\n",
    "            logger.info(f\"{feat}: {score:.3f}\")\n",
    "        \n",
    "        # Monitor emission parameters\n",
    "        logger.info(\"\\nEmission Parameter Stability:\")\n",
    "        logger.info(f\"Mean change: {iter_stats['param_stability']['mean_change']:.3e}\")\n",
    "        logger.info(f\"Max change: {iter_stats['param_stability']['max_change']:.3e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    def _adjust_learning_rate(self, iter_stats: Dict) -> None:\n",
    "        \"\"\"Adaptive learning rate adjustment with warmup and stability control\"\"\"\n",
    "        # Initialize tracking if not exists\n",
    "        if not hasattr(self, '_lr_tracking'):\n",
    "            self._lr_tracking = { 'warmup_steps': 10, 'min_lr': 1e-5, 'max_lr': self.config.learning_rate * 2.0, 'best_ll': float('-inf'), 'no_improve_count': 0 }\n",
    "        \n",
    "        # Get current likelihood and state distribution\n",
    "        current_ll = iter_stats['total_ll']\n",
    "        state_dist = iter_stats['state_dist']\n",
    "        iteration = len(self.history['train_ll']) if hasattr(self, 'history') else 0\n",
    "        \n",
    "        # Basic warmup phase\n",
    "        if iteration < self._lr_tracking['warmup_steps']:\n",
    "            warmup_factor = (iteration + 1) / self._lr_tracking['warmup_steps']\n",
    "            self.config.learning_rate *= warmup_factor\n",
    "            return\n",
    "        \n",
    "        # Compute adaptive factors\n",
    "        stability_factor = 1.0\n",
    "        balance_factor = 1.0\n",
    "        \n",
    "        # State balance based adjustment\n",
    "        min_state_prob = np.min(state_dist)\n",
    "        if min_state_prob < self.config.min_state_prob:\n",
    "            balance_factor = min_state_prob / self.config.min_state_prob\n",
    "        \n",
    "        # Performance based adjustment\n",
    "        if current_ll > self._lr_tracking['best_ll']:\n",
    "            self._lr_tracking['best_ll'] = current_ll\n",
    "            self._lr_tracking['no_improve_count'] = 0\n",
    "        else:\n",
    "            self._lr_tracking['no_improve_count'] += 1\n",
    "            stability_factor = 0.95 ** self._lr_tracking['no_improve_count']\n",
    "        \n",
    "        # Parameter stability check\n",
    "        if 'param_stability' in iter_stats and iter_stats['param_stability']['max_change'] > self.config.emission_config['update_clip']:\n",
    "            stability_factor *= 0.9\n",
    "        \n",
    "        # Apply all adjustments\n",
    "        self.config.learning_rate *= (\n",
    "            self.config.lr_decay *      # Base decay\n",
    "            stability_factor *          # Stability adjustment\n",
    "            np.sqrt(balance_factor)     # State balance influence\n",
    "        )\n",
    "        \n",
    "        # Bound the learning rate\n",
    "        self.config.learning_rate = np.clip( self.config.learning_rate, self._lr_tracking['min_lr'], self._lr_tracking['max_lr'] )\n",
    "        \n",
    "        # Log significant changes\n",
    "        if stability_factor < 0.95 or balance_factor < 0.9:\n",
    "            logger.debug(f\"Learning rate adjusted: {self.config.learning_rate:.6f} \"\n",
    "                        f\"(stability: {stability_factor:.3f}, balance: {balance_factor:.3f})\")\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "    def _log_iteration_details(self, iter_stats: Dict, iteration: int, has_validation: bool) -> None:\n",
    "        \"\"\"Helper to log iteration details in a clean format\"\"\"\n",
    "        state_dist = iter_stats['state_dist']\n",
    "        logger.info(f\"\\nIteration {iteration}:\")\n",
    "        logger.info(f\"Training Log-Likelihood: {iter_stats['total_ll']:.2f}\")\n",
    "        logger.info(f\"State Distribution: {[f'{p:.3f}' for p in state_dist]}\")\n",
    "        logger.info(f\"Learning Rate: {self.config.learning_rate:.6f}\")\n",
    "        \n",
    "        if has_validation:\n",
    "            val_stats = iter_stats['val_stats']\n",
    "            logger.info(f\"Validation Log-Likelihood: {val_stats['val_ll']:.2f}\")\n",
    "            logger.info(f\"Validation Accuracy: {val_stats['val_acc']:.3f}\")\n",
    "\n",
    "\n",
    "        \n",
    "    def _initialize_history(self, has_validation: bool) -> Dict[str, List]:\n",
    "        \"\"\"Initialize history dictionary with required tracking lists\"\"\"\n",
    "        return { 'train_ll': [], 'val_ll': [] if has_validation else None, 'val_accuracy': [] if has_validation else None, 'state_usage': [], 'transition_diag': [] }\n",
    "    \n",
    "\n",
    "    ## sequence validation → feature extraction → emission computation → forward/backward → statistics collection → parameter updates\n",
    "    def _process_training_iteration(self, iteration: int, sequences: List[np.ndarray], history: Dict[str, List], val_sequences: Optional[List[np.ndarray]], val_labels: Optional[List[np.ndarray]]) -> Dict:\n",
    "        \"\"\"Process training iteration with enhanced logging and monitoring\"\"\"\n",
    "        # Collect training statistics\n",
    "        train_stats = self._collect_statistics(sequences)\n",
    "        total_ll = train_stats['log_likelihood']\n",
    "        \n",
    "        # Maintain existing state monitoring with proper handling\n",
    "        state_usage = train_stats['state_occurences']\n",
    "        total_valid = np.sum(state_usage) + self.config.stability_config['balance_threshold']\n",
    "        state_dist = state_usage / total_valid\n",
    "        \n",
    "        # Preserve existing history tracking\n",
    "        history['train_ll'].append(total_ll)\n",
    "        history['state_usage'].append(state_dist)\n",
    "        history['transition_diag'].append(np.diag(self.transitions).copy())\n",
    "        \n",
    "        # Keep emission range tracking\n",
    "        history.setdefault('emission_ranges', []).append([\n",
    "            (self.emission_means[i].min(), self.emission_means[i].max())\n",
    "            for i in range(self.config.n_states)\n",
    "        ])\n",
    "        \n",
    "        # Maintain existing logging with proper frequency\n",
    "        logger.info(f\"\\nIteration {iteration}:\")\n",
    "        logger.info(f\"Training Log-Likelihood: {total_ll:.2f}\")\n",
    "        logger.info(f\"State Distribution: {[f'{p:.3f}' for p in state_dist]}\")\n",
    "        logger.info(f\"Learning Rate: {self.config.learning_rate:.6f}\")\n",
    "        \n",
    "        # Keep validation handling with all its logic\n",
    "        val_stats = None\n",
    "        if val_sequences:\n",
    "            val_stats = self._compute_validation_metrics(val_sequences, val_labels)\n",
    "            history['val_ll'].append(val_stats['val_ll'])\n",
    "            history['val_accuracy'].append(val_stats['val_acc'])\n",
    "            \n",
    "            logger.info(f\"Validation Log-Likelihood: {val_stats['val_ll']:.2f}\")\n",
    "            logger.info(f\"Validation Accuracy: {val_stats['val_acc']:.3f}\")\n",
    "        \n",
    "        # Preserve detailed state analysis\n",
    "        if iteration % self.config.logging_config['logging_frequency'] == 0:\n",
    "            logger.info(\"\\nDetailed State Analysis:\")\n",
    "            for state in range(self.config.n_states):\n",
    "                stats = self._debug_state_statistics(sequences[0], state)\n",
    "                logger.info(f\"\\nState {state}:\")\n",
    "                logger.info(f\"  Mean Emission: {stats['mean_prob']:.3f}\")\n",
    "                logger.info(f\"  Transitions - In: {stats['incoming_trans']}\")\n",
    "                logger.info(f\"  Transitions - Out: {stats['outgoing_trans']}\")\n",
    "                logger.info(f\"  Self-Transition: {stats['self_trans']:.3f}\")\n",
    "        \n",
    "        # Keep state collapse checking with time-based warning control\n",
    "        if np.any(state_dist < self.config.min_state_prob):\n",
    "            curr_time = time.time()\n",
    "            if not hasattr(self, '_last_collapse_warning_time'):\n",
    "                self._last_collapse_warning_time = 0\n",
    "                \n",
    "            if curr_time - self._last_collapse_warning_time >= self.config.logging_config['min_warning_interval']:\n",
    "                logger.warning(f\"State collapse detected: {[f'{p:.4f}' for p in state_dist]}\")\n",
    "                self._log_collapse_warning(state_dist, sequences[0])\n",
    "                self._last_collapse_warning_time = curr_time\n",
    "        \n",
    "        # Integrate mixture monitoring without losing existing stats\n",
    "        mixture_monitoring = {}\n",
    "        if 'mixture_monitoring' in train_stats:\n",
    "            mixture_monitoring = train_stats['mixture_monitoring']\n",
    "        \n",
    "        # Return with all existing keys plus mixture monitoring\n",
    "        return {\n",
    "            'train_stats': train_stats,\n",
    "            'val_stats': val_stats,\n",
    "            'state_dist': state_dist,\n",
    "            'total_ll': total_ll,\n",
    "            'mixture_monitoring': mixture_monitoring\n",
    "        }    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def _compute_validation_metrics(self, val_sequences: List[np.ndarray], \n",
    "                                  val_labels: List[np.ndarray]) -> Dict:\n",
    "        \"\"\"Compute validation metrics during training\"\"\"\n",
    "        val_ll = sum(self.score(x) for x in val_sequences)\n",
    "        val_pred = self.predict_batch(val_sequences)\n",
    "        val_true = [np.argmax(label, axis=1) for label in val_labels]\n",
    "        val_acc = np.mean([np.mean(p == t) for p, t in zip(val_pred, val_true)])\n",
    "        \n",
    "        return {'val_ll': val_ll, 'val_acc': val_acc}\n",
    "\n",
    "\n",
    "    def _log_iteration_stats(self, iteration: int, train_ll: float, \n",
    "                            val_stats: Dict, state_dist: np.ndarray) -> None:\n",
    "        \"\"\"Log training iteration statistics\"\"\"\n",
    "        logger.info(f\"Iteration {iteration}:\")\n",
    "        logger.info(f\"  Train LL: {train_ll:.2f}\")\n",
    "        logger.info(f\"  Val LL: {val_stats['val_ll']:.2f}\")\n",
    "        logger.info(f\"  Val Acc: {val_stats['val_acc']:.3f}\")\n",
    "        logger.info(f\"  State Distribution: {[f'{p:.3f}' for p in state_dist]}\")\n",
    "        \n",
    "    \n",
    "    def _check_convergence(self, iter_stats: Dict, best_ll: float, best_params: Optional[Dict],\n",
    "                          no_improve: int, plateau_count: int, tolerance: float) -> Tuple[bool, float, Optional[Dict]]:\n",
    "        \"\"\"Modified convergence check with more tolerant early stopping\"\"\"\n",
    "        should_stop = False\n",
    "        val_stats = iter_stats['val_stats']\n",
    "        current_iteration = len(self.history['train_ll']) if hasattr(self, 'history') else 0\n",
    "        \n",
    "        # Allow some initial iterations before checking state collapse\n",
    "        min_iterations = 5  # Give at least 5 iterations to stabilize\n",
    "        \n",
    "        if val_stats:\n",
    "            if val_stats['val_ll'] > best_ll + tolerance:\n",
    "                best_ll = val_stats['val_ll']\n",
    "                best_params = self._get_model_params()\n",
    "                no_improve = 0\n",
    "                plateau_count = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if abs(val_stats['val_ll'] - best_ll) < tolerance:\n",
    "                    plateau_count += 1\n",
    "                else:\n",
    "                    plateau_count = 0\n",
    "            \n",
    "            # Check stopping criteria with more tolerance\n",
    "            if current_iteration >= min_iterations:\n",
    "                state_dist = iter_stats['state_dist']\n",
    "                if no_improve >= 10:  # Increased from 5\n",
    "                    logger.info(\"Early stopping: No improvement for 10 iterations\")\n",
    "                    should_stop = True\n",
    "                elif plateau_count >= 5:  # Increased from 3\n",
    "                    logger.info(\"Early stopping: Detected convergence plateau\")\n",
    "                    should_stop = True\n",
    "                elif np.any(state_dist < self.config.min_state_prob) and best_params:\n",
    "                    # Check if state collapse persists for multiple iterations\n",
    "                    if hasattr(self, '_collapse_counter'):\n",
    "                        self._collapse_counter += 1\n",
    "                    else:\n",
    "                        self._collapse_counter = 1\n",
    "                    \n",
    "                    if self._collapse_counter >= 3:  # Require persistent collapse\n",
    "                        logger.info(\"Early stopping: Persistent state collapse detected\")\n",
    "                        should_stop = True\n",
    "                else:\n",
    "                    self._collapse_counter = 0  # Reset counter if distribution improves\n",
    "        \n",
    "        return should_stop, best_ll, best_params\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def _log_state_analysis(self, sample_sequence: np.ndarray) -> None:\n",
    "        \"\"\"Log detailed state analysis\"\"\"\n",
    "        logger.info(\"\\nDetailed State Analysis:\")\n",
    "        for state in range(self.config.n_states):\n",
    "            stats = self._debug_state_statistics(sample_sequence, state)\n",
    "            logger.info(f\"\\nState {state}: Emission mean={stats['mean_prob']:.3f}, \"\n",
    "                       f\"std={stats['std_prob']:.3f}\")\n",
    "            logger.info(f\"Transitions - In: {stats['incoming_trans']}, \"\n",
    "                       f\"Out: {stats['outgoing_trans']}\")\n",
    "    \n",
    "    def _log_collapse_warning(self, state_dist: np.ndarray, sample_sequence: np.ndarray) -> None:\n",
    "        \"\"\"Log detailed warning when state collapse is detected\"\"\"\n",
    "        logger.warning(f\"State collapse detected: {[f'{p:.4f}' for p in state_dist]}\")\n",
    "        logger.warning(\"State Details:\")\n",
    "        for state in range(self.config.n_states):\n",
    "            if state_dist[state] < self.config.min_state_prob:\n",
    "                stats = self._debug_state_statistics(sample_sequence, state)\n",
    "                logger.warning(f\"Collapsed State {state}:\")\n",
    "                logger.warning(f\"  - Mean Emission: {stats['mean_prob']:.4f}\")\n",
    "                logger.warning(f\"  - Incoming Trans: {stats['incoming_trans']}\")\n",
    "                logger.warning(f\"  - Self Trans: {stats['self_trans']:.4f}\")    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    ## Collect Statistics Method\n",
    "    ## Critical Issue: Was accumulating statistics for all positions\n",
    "    ## Fix: Now only processes valid sequence positions\n",
    "    def _collect_statistics(self, sequences: List[np.ndarray]) -> Dict:\n",
    "        \"\"\"Collect sufficient statistics with enhanced monitoring and validation\"\"\"\n",
    "        stats = {\n",
    "            'transition_counts': np.zeros((self.config.n_states, self.config.n_states)), \n",
    "            'emission_stats': {\n",
    "                'means_num': np.zeros((self.config.n_states, self.config.n_mixtures, self.config.n_features)),\n",
    "                'covs_num': np.zeros((self.config.n_states, self.config.n_mixtures, self.config.n_features)),\n",
    "                'weights_num': np.zeros((self.config.n_states, self.config.n_mixtures)),\n",
    "                'feature_contributions': np.zeros((self.config.n_states, len(self.config.feature_config))),\n",
    "                'component_usage': np.zeros((self.config.n_states, self.config.n_mixtures))\n",
    "            },\n",
    "            'state_occurences': np.zeros(self.config.n_states),\n",
    "            'log_likelihood': 0.0,\n",
    "            'sequence_stats': [],\n",
    "            'stability_metrics': {'forward': [], 'backward': [], 'mixture': []}\n",
    "        }\n",
    "        \n",
    "        for seq_idx, x in enumerate(sequences):\n",
    "            ## valid: one_hot = x[:, :21]; valid_pos = np.where(np.sum(one_hot, axis=1) > 0)[0]\n",
    "            valid_pos = self._get_valid_positions(x)\n",
    "            if len(valid_pos) == 0: logger.warning(f\"Skipping empty sequence {seq_idx}\"); continue\n",
    "                \n",
    "            # Forward-backward pass with full statistics collection\n",
    "            x_valid = x[valid_pos]; alpha, seq_ll, forward_stats = self.forward(x_valid)\n",
    "            if np.isnan(seq_ll) or np.isinf(seq_ll): logger.warning(f\"Invalid log-likelihood for sequence {seq_idx}, skipping\"); continue\n",
    "            \n",
    "            stats['log_likelihood'] += seq_ll\n",
    "            beta, backward_stats = self.backward(x_valid, alpha.sum(axis=1))\n",
    "            posteriors = self.compute_posteriors(alpha, beta)\n",
    "\n",
    "\n",
    "            # Update sequence-level statistics\n",
    "            seq_stats = {'length': len(valid_pos), 'state_dist': posteriors.sum(axis=0) / len(valid_pos), \n",
    "                         'forward_stats': forward_stats, 'backward_stats': backward_stats}\n",
    "            stats['sequence_stats'].append(seq_stats)\n",
    "\n",
    "            # Properly access nested stability metrics\n",
    "            stats['stability_metrics']['forward'].extend(forward_stats['detailed']['stability_metrics'])\n",
    "            if 'detailed' in backward_stats and 'stability_metrics' in backward_stats['detailed']:\n",
    "                stats['stability_metrics']['backward'].extend(backward_stats['detailed']['stability_metrics'])\n",
    "                \n",
    "            # Update transition and emission statistics with mixture responsibilities\n",
    "            stats['state_occurences'] += posteriors.sum(axis=0)\n",
    "            for t in range(len(valid_pos) - 1):\n",
    "                for i in range(self.config.n_states):\n",
    "                    for j in range(self.config.n_states):\n",
    "                        trans_prob = posteriors[t, i] * self.transitions[i, j] * self._compute_emission_likelihood(x_valid[t+1:t+2])[0, j] * posteriors[t+1, j]\n",
    "                        stats['transition_counts'][i, j] += trans_prob\n",
    "            \n",
    "            # Update emission statistics with mixture information\n",
    "            self._update_emission_statistics(x_valid, posteriors, stats['emission_stats'])\n",
    "            \n",
    "            if seq_idx % 100 == 0: logger.debug(f\"Processing sequence {seq_idx}, length={len(valid_pos)}\")\n",
    "        \n",
    "        return stats\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def _update_sequence_statistics(self, x: np.ndarray, posteriors: np.ndarray, stats: Dict) -> None:\n",
    "        \"\"\"Update sufficient statistics for a single sequence\"\"\"\n",
    "        seq_len = len(x)\n",
    "        emission_probs = self._compute_emission_likelihood(x)\n",
    "        \n",
    "        # Update state occurrences\n",
    "        stats['state_occurences'] += posteriors.sum(axis=0)\n",
    "        \n",
    "        # Update transition counts\n",
    "        for t in range(seq_len - 1):\n",
    "            for i in range(self.config.n_states):\n",
    "                for j in range(self.config.n_states):\n",
    "                    stats['transition_counts'][i, j] += (\n",
    "                        posteriors[t, i] * self.transitions[i, j] * \n",
    "                        emission_probs[t+1, j] * posteriors[t+1, j]\n",
    "                    )\n",
    "        \n",
    "        # Update emission statistics\n",
    "        self._update_emission_statistics(x, posteriors, stats['emission_stats'])\n",
    "\n",
    "    \n",
    "    ## Update Emission Statistics\n",
    "    ## Critical Issue: Was processing features incorrectly\n",
    "    ## Fix: Now properly handles feature ranges and valid positions\n",
    "    def _update_emission_statistics(self, x: np.ndarray, posteriors: np.ndarray, emission_stats: Dict) -> None:\n",
    "        \"\"\"Update emission statistics with proper feature handling and mixture tracking\"\"\"\n",
    "\n",
    "        valid_pos = self._get_valid_positions(x)\n",
    "        x_valid = x[valid_pos]\n",
    "        x = x_valid\n",
    "        for state in range(self.config.n_states):\n",
    "            # Get mixture responsibilities and track usage patterns\n",
    "            mix_resp, mix_stats = self._compute_mixture_responsibilities(x, state, posteriors[:, state])\n",
    "            emission_stats['component_usage'][state] += mix_stats['dominance_patterns']\n",
    "\n",
    "            feature_sizes = {\n",
    "                    'one_hot': 21,  # 21 amino acids\n",
    "                    'pssm': 21,    # 21 PSSM scores\n",
    "                    'aux': 5       # 5 auxiliary features\n",
    "                }\n",
    "                \n",
    "            # Update per-mixture statistics with feature-specific handling\n",
    "            for mix in range(self.config.n_mixtures):\n",
    "                resp = mix_resp[:, mix].reshape(-1, 1)\n",
    "                emission_stats['weights_num'][state, mix] += resp.sum()\n",
    "                \n",
    "                # Update means and covariances for each feature type\n",
    "                curr_pos = 0\n",
    "                for feat_type, size in feature_sizes.items():\n",
    "                    feat_slice = slice(curr_pos, curr_pos + size)\n",
    "                    # Update means\n",
    "                    emission_stats['means_num'][state, mix, feat_slice] += (resp * x[:, feat_slice]).sum(axis=0)\n",
    "                    # Update covariances\n",
    "                    diff = x[:, feat_slice] - self.emission_means[state, mix, feat_slice]\n",
    "                    emission_stats['covs_num'][state, mix, feat_slice] += (resp * (diff ** 2)).sum(axis=0)\n",
    "                    # Update feature contributions\n",
    "                    emission_stats['feature_contributions'][state] += mix_stats['feature_contributions'][feat_type][mix]\n",
    "                    \n",
    "                    curr_pos += size\n",
    "    \n",
    "\n",
    "    \n",
    "    def _get_model_params(self) -> Dict:\n",
    "        \"\"\"Get current model parameters for checkpointing\"\"\"\n",
    "        return {\n",
    "            'transitions': self.transitions.copy(),\n",
    "            'emission_means': self.emission_means.copy(),\n",
    "            'emission_covs': self.emission_covs.copy(),\n",
    "            'mixture_weights': self.mixture_weights.copy()\n",
    "        }\n",
    "\n",
    "    def _restore_parameters(self, params: Dict) -> None:\n",
    "        \"\"\"Restore model parameters from checkpoint\"\"\"\n",
    "        self.transitions = params['transitions']\n",
    "        self.emission_means = params['emission_means']\n",
    "        self.emission_covs = params['emission_covs']\n",
    "        self.mixture_weights = params['mixture_weights']\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"Compute log-likelihood score for a sequence\"\"\"\n",
    "    def score(self, x: np.ndarray) -> float:\n",
    "        # Forward pass to get likelihood\n",
    "        ## _, log_likelihood = ....\n",
    "        \n",
    "        alpha, log_likelihood, state_probs = self.forward(x)\n",
    "\n",
    "        # Check for numerical issues\n",
    "        if np.isnan(log_likelihood) or np.isinf(log_likelihood):\n",
    "            logger.warning(f\"Invalid log-likelihood: {log_likelihood}\")\n",
    "            return float('-inf')\n",
    "        \n",
    "        return log_likelihood\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    def predict(self, x: np.ndarray, method: str = 'viterbi') -> np.ndarray:\n",
    "        \"\"\"Predict secondary structure states with confidence tracking\"\"\"\n",
    "        # Validate and process sequence\n",
    "        one_hot = x[:, :21]; valid_pos = np.where(np.sum(one_hot, axis=1) > 0)[0]\n",
    "        if len(valid_pos) == 0: raise ValueError(\"Empty sequence provided\")\n",
    "        x_valid = x[valid_pos]; prediction_stats = {'confidence': [], 'method': method, 'sequence_length': len(valid_pos)}\n",
    "        \n",
    "        # Get predictions based on method\n",
    "        if method == 'viterbi':\n",
    "            path, score, path_stats = self.viterbi(x_valid)\n",
    "            predictions = np.array(path)\n",
    "            prediction_stats.update({'path_score': score, 'path_stats': path_stats, 'confidence': path_stats['path_probabilities']})\n",
    "        elif method == 'posterior':\n",
    "            alpha, _, forward_stats = self.forward(x_valid)\n",
    "            beta, backward_stats = self.backward(x_valid, alpha.sum(axis=1))\n",
    "            posteriors = self.compute_posteriors(alpha, beta)\n",
    "            predictions = np.argmax(posteriors, axis=1)\n",
    "            prediction_stats.update({'posteriors': posteriors, 'max_probs': np.max(posteriors, axis=1), 'confidence': np.max(posteriors, axis=1)})\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction method: {method}\")\n",
    "        \n",
    "        # Monitor prediction confidence\n",
    "        mean_conf = np.mean(prediction_stats['confidence'])\n",
    "        if mean_conf < 0.5: logger.warning(f\"Low prediction confidence: {mean_conf:.3f}\")\n",
    "        \n",
    "        return predictions, prediction_stats\n",
    "\n",
    "    \n",
    "        \n",
    "    def predict_batch(self, sequences: List[np.ndarray], method: str = 'viterbi') -> Tuple[List[np.ndarray], Dict]:\n",
    "        \"\"\"Batch prediction with comprehensive statistics tracking\"\"\"\n",
    "        predictions = []; batch_stats = {'confidence': [], 'lengths': [], 'method_stats': {method: {'success': 0, 'failed': 0}}}; logger.info(f\"Processing batch of {len(sequences)} sequences\")\n",
    "        \n",
    "        for i, seq in enumerate(sequences):\n",
    "            try:\n",
    "                pred, pred_stats = self.predict(seq, method)\n",
    "                predictions.append(pred)\n",
    "                batch_stats['confidence'].append(pred_stats['confidence'])\n",
    "                batch_stats['lengths'].append(pred_stats['sequence_length'])\n",
    "                batch_stats['method_stats'][method]['success'] += 1\n",
    "                \n",
    "                if i % 100 == 0: logger.debug(f\"Processed {i + 1} sequences, avg confidence: {np.mean(batch_stats['confidence'][-100:]):.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to process sequence {i}: {str(e)}\")\n",
    "                batch_stats['method_stats'][method]['failed'] += 1\n",
    "                continue\n",
    "        \n",
    "        # Compute batch statistics\n",
    "        batch_stats.update({\n",
    "            'mean_confidence': np.mean([np.mean(conf) for conf in batch_stats['confidence']]),\n",
    "            'sequence_length_stats': {'mean': np.mean(batch_stats['lengths']), 'std': np.std(batch_stats['lengths'])},\n",
    "            'success_rate': batch_stats['method_stats'][method]['success'] / len(sequences)\n",
    "        })\n",
    "        \n",
    "        if batch_stats['mean_confidence'] < 0.6: logger.warning(f\"Low average batch confidence: {batch_stats['mean_confidence']:.3f}\")\n",
    "        \n",
    "        return predictions, batch_stats\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate(self, sequences: List[np.ndarray], true_labels: List[np.ndarray]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model with comprehensive metrics and state-wise analysis\"\"\"\n",
    "        predictions, batch_stats = self.predict_batch(sequences); metrics = {}\n",
    "        \n",
    "        # Flatten predictions and labels with proper handling\n",
    "        y_pred = np.concatenate(predictions); y_true = np.concatenate([np.argmax(label, axis=1) if label.ndim > 1 else label for label in true_labels])\n",
    "        state_metrics = {state: {'tp': 0, 'fp': 0, 'fn': 0} for state in range(self.config.n_states)}\n",
    "        \n",
    "        # Compute metrics with state-wise tracking\n",
    "        metrics['accuracy'] = np.mean(y_pred == y_true)\n",
    "        for state in range(self.config.n_states):\n",
    "            state_metrics[state]['tp'] = np.sum((y_pred == state) & (y_true == state))\n",
    "            state_metrics[state]['fp'] = np.sum((y_pred == state) & (y_true != state))\n",
    "            state_metrics[state]['fn'] = np.sum((y_pred != state) & (y_true == state))\n",
    "            metrics[f'state_{state}_precision'] = state_metrics[state]['tp'] / (state_metrics[state]['tp'] + state_metrics[state]['fp'] + 1e-10)\n",
    "            metrics[f'state_{state}_recall'] = state_metrics[state]['tp'] / (state_metrics[state]['tp'] + state_metrics[state]['fn'] + 1e-10)\n",
    "            metrics[f'state_{state}_f1'] = 2 * metrics[f'state_{state}_precision'] * metrics[f'state_{state}_recall'] / (metrics[f'state_{state}_precision'] + metrics[f'state_{state}_recall'] + 1e-10)\n",
    "        \n",
    "        # Log results and warnings\n",
    "        logger.info(f\"\\nEvaluation Results:\\nAccuracy: {metrics['accuracy']:.3f}\")\n",
    "        for state, name in enumerate(['Helix', 'Sheet', 'Coil']): logger.info(f\"{name} F1: {metrics[f'state_{state}_f1']:.3f}\")\n",
    "        if any(metrics[f'state_{s}_f1'] < 0.3 for s in range(self.config.n_states)): logger.warning(\"Low performance detected for some states\")\n",
    "        \n",
    "        return {**metrics, 'batch_stats': batch_stats}\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def save_model(self, filepath: str) -> None:\n",
    "        \"\"\"Save model parameters and stats with version tracking\"\"\"\n",
    "        save_dict = {\n",
    "            'config': self.config,\n",
    "            'state_priors': self.state_priors,\n",
    "            'transitions': self.transitions,\n",
    "            'emission_means': self.emission_means,\n",
    "            'emission_covs': self.emission_covs,\n",
    "            'mixture_weights': self.mixture_weights,\n",
    "            'model_stats': {\n",
    "                'state_distributions': np.diag(self.transitions).copy(),\n",
    "                'emission_ranges': {'means': (self.emission_means.min(), self.emission_means.max()), 'covs': (self.emission_covs.min(), self.emission_covs.max())},\n",
    "                'timestamp': datetime.now().strftime('%Y%m%d_%H%M')\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(save_dict, filepath)\n",
    "        logger.info(f\"Model saved to {filepath} with state distribution: {[f'{p:.3f}' for p in np.diag(self.transitions)]}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath: str) -> 'MixtureGaussianHMM':\n",
    "        \"\"\"Load model with parameter validation\"\"\"\n",
    "        logger.info(f\"Loading model from {filepath}\")\n",
    "        saved_dict = joblib.load(filepath)\n",
    "        \n",
    "        # Validate loaded parameters\n",
    "        required_keys = ['config', 'state_priors', 'transitions', 'emission_means', 'emission_covs', 'mixture_weights']\n",
    "        if not all(key in saved_dict for key in required_keys):\n",
    "            raise ValueError(\"Invalid model file: missing required parameters\")\n",
    "        \n",
    "        # Initialize and restore model\n",
    "        model = cls(saved_dict['config'])\n",
    "        model.state_priors = saved_dict['state_priors']\n",
    "        model.transitions = saved_dict['transitions']\n",
    "        model.emission_means = saved_dict['emission_means']\n",
    "        model.emission_covs = saved_dict['emission_covs']\n",
    "        model.mixture_weights = saved_dict['mixture_weights']\n",
    "        \n",
    "        # Validate parameter shapes and distributions\n",
    "        if np.any(model.transitions < 0) or not np.allclose(model.transitions.sum(axis=1), 1.0):\n",
    "            logger.warning(\"Loaded transitions may be invalid\")\n",
    "        if np.any(model.emission_covs <= 0):\n",
    "            logger.warning(\"Invalid emission covariances detected\")\n",
    "            \n",
    "        stats = saved_dict.get('model_stats', {})\n",
    "        logger.info(f\"Model loaded with state distribution: {[f'{p:.3f}' for p in stats.get('state_distributions', np.diag(model.transitions))]}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    \n",
    "## ------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_npy_file(input_path: str, output_path: str = None) -> str:\n",
    "    \"\"\"Convert Python 2 NPY file to Python 3 format and save\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = input_path.replace('.npy', '_py3.npy')\n",
    "    \n",
    "    logger.info(f\"Converting NPY file from Python 2 to Python 3 format\")\n",
    "    # Load and immediately save in new format\n",
    "    data = np.load(input_path)\n",
    "    np.save(output_path, data)\n",
    "    logger.info(f\"Saved converted file to {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "\n",
    "def visualize_state_predictions(model: 'MixtureGaussianHMM', sequence: np.ndarray, true_labels: np.ndarray = None, save_path: str = None) -> None:\n",
    "    \"\"\"Visualize predictions with state posteriors and confidence\"\"\"\n",
    "    predictions, pred_stats = model.predict(sequence); posteriors = pred_stats.get('posteriors', model.compute_posteriors(*model.forward(sequence)[:2]))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), gridspec_kw={'height_ratios': [1, 3]}); plt.tight_layout(pad=3.0)\n",
    "    \n",
    "    # Plot predictions with confidence bands\n",
    "    ax1.plot(predictions, 'k-', label='Predicted', alpha=0.7)\n",
    "    if true_labels is not None: ax1.plot(true_labels if true_labels.ndim == 1 else np.argmax(true_labels, axis=1), 'r--', label='True', alpha=0.5)\n",
    "    ax1.fill_between(range(len(predictions)), predictions - 0.2, predictions + 0.2, alpha=0.2, color='gray', label='Confidence')\n",
    "    ax1.set_title('Structure Predictions with Confidence'); ax1.set_ylabel('State'); ax1.legend()\n",
    "    \n",
    "    # Plot state posteriors heatmap\n",
    "    im = ax2.imshow(posteriors.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "    ax2.set_title('State Posteriors'); ax2.set_xlabel('Position'); ax2.set_ylabel('State')\n",
    "    plt.colorbar(im, ax=ax2, label='Probability')\n",
    "    \n",
    "    if save_path: plt.savefig(save_path); logger.info(f\"Saved prediction visualization to {save_path}\")\n",
    "    plt.show()  # Display plot\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_training_progress(history: Dict[str, List], save_path: str = None) -> None:\n",
    "    \"\"\"Plot training metrics in a concise layout\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10)); plt.tight_layout(pad=3.0)\n",
    "    \n",
    "    # Log likelihood progress\n",
    "    ax1.plot(history['train_ll'], label='Train', color='blue', alpha=0.7)\n",
    "    if history.get('val_ll'): ax1.plot(history['val_ll'], label='Validation', color='red', alpha=0.7)\n",
    "    ax1.set_title('Log Likelihood'); ax1.set_xlabel('Iteration'); ax1.set_ylabel('LL'); ax1.legend()\n",
    "    \n",
    "    # State distribution evolution\n",
    "    state_usage = np.array(history['state_usage'])\n",
    "    for i, label in enumerate(['Helix', 'Sheet', 'Coil']): ax2.plot(state_usage[:, i], label=label, alpha=0.7)\n",
    "    ax2.axhline(y=0.15, color='r', linestyle='--', alpha=0.3, label='Min Threshold')\n",
    "    ax2.set_title('State Distribution'); ax2.set_xlabel('Iteration'); ax2.set_ylabel('Probability'); ax2.legend()\n",
    "    \n",
    "    # Self-transition evolution\n",
    "    trans_diag = np.array(history['transition_diag'])\n",
    "    for i, label in enumerate(['H→H', 'E→E', 'C→C']): ax3.plot(trans_diag[:, i], label=label, alpha=0.7)\n",
    "    ax3.set_title('Self-Transitions'); ax3.set_xlabel('Iteration'); ax3.set_ylabel('Probability'); ax3.legend()\n",
    "    \n",
    "    # Emission parameter ranges\n",
    "    emission_ranges = np.array(history['emission_ranges'])\n",
    "    for i, label in enumerate(['Helix', 'Sheet', 'Coil']): \n",
    "        means = np.mean([r[i][1] - r[i][0] for r in emission_ranges])\n",
    "        ax4.plot([r[i][1] - r[i][0] for r in emission_ranges], label=f'{label} (μ={means:.2f})', alpha=0.7)\n",
    "    ax4.set_title('Emission Spread'); ax4.set_xlabel('Iteration'); ax4.set_ylabel('Range'); ax4.legend()\n",
    "    \n",
    "    if save_path: plt.savefig(save_path); logger.info(f\"Saved training visualization to {save_path}\")\n",
    "    plt.show()  # Display plot    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(train_seqs: List[np.ndarray], val_seqs: List[np.ndarray], train_labels: List[np.ndarray], val_labels: List[np.ndarray], config: ModelConfig) -> Tuple[MixtureGaussianHMM, Dict]:\n",
    "    \"\"\"Train and evaluate model with validation\"\"\"\n",
    "    model = MixtureGaussianHMM(config); logger.info(\"Starting model training...\")\n",
    "    history = model.train(train_seqs, val_sequences=val_seqs, val_labels=val_labels, n_iterations=100)\n",
    "    val_metrics = model.evaluate(val_seqs, val_labels)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    model.save_model(f'models/hmm_model_{timestamp}.joblib')\n",
    "    return model, val_metrics\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_detailed(model: MixtureGaussianHMM, sequences: List[np.ndarray], \n",
    "                     labels: List[np.ndarray]) -> Dict[str, float]:\n",
    "    \"\"\"Enhanced evaluation with detailed metrics and state analysis\"\"\"\n",
    "    predictions = model.predict_batch(sequences)\n",
    "    metrics = {}\n",
    "    \n",
    "    # Convert one-hot labels to indices if needed\n",
    "    processed_labels = []\n",
    "    for label in labels:\n",
    "        if label.ndim == 2 and label.shape[1] > 1:\n",
    "            processed_labels.append(np.argmax(label, axis=1))\n",
    "        else:\n",
    "            processed_labels.append(label)\n",
    "    \n",
    "    # Overall metrics\n",
    "    y_pred = np.concatenate(predictions)\n",
    "    y_true = np.concatenate(processed_labels)\n",
    "    metrics['accuracy'] = np.mean(y_pred == y_true)\n",
    "    \n",
    "    # Per-state metrics (including F1)\n",
    "    state_names = ['Helix', 'Sheet', 'Coil']\n",
    "    for state, name in enumerate(state_names):\n",
    "        true_pos = np.sum((y_pred == state) & (y_true == state))\n",
    "        false_pos = np.sum((y_pred == state) & (y_true != state))\n",
    "        false_neg = np.sum((y_pred != state) & (y_true == state))\n",
    "        \n",
    "        precision = true_pos / (true_pos + false_pos + 1e-10)\n",
    "        recall = true_pos / (true_pos + false_neg + 1e-10)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        \n",
    "        metrics[f'{name}_precision'] = precision\n",
    "        metrics[f'{name}_recall'] = recall\n",
    "        metrics[f'{name}_f1'] = f1\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_metrics(metrics: Dict[str, float], save_path: str = 'results') -> None:\n",
    "    \"\"\"Save evaluation metrics to file\"\"\"\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    \n",
    "    with open(f'{save_path}/metrics_{timestamp}.txt', 'w') as f:\n",
    "        f.write(\"=== Protein Structure Prediction Results ===\\n\\n\")\n",
    "        \n",
    "        # Overall metrics\n",
    "        f.write(f\"Overall Accuracy: {metrics['accuracy']:.3f}\\n\")\n",
    "        f.write(f\"Mean Sequence Accuracy: {metrics['mean_seq_accuracy']:.3f} (±{metrics['std_seq_accuracy']:.3f})\\n\")\n",
    "        f.write(f\"Sequence Accuracy Range: [{metrics['min_seq_accuracy']:.3f}, {metrics['max_seq_accuracy']:.3f}]\\n\\n\")\n",
    "        \n",
    "        # Per-state metrics\n",
    "        f.write(\"Per-State Metrics:\\n\")\n",
    "        for state in ['Helix', 'Sheet', 'Coil']:\n",
    "            f.write(f\"\\n{state}:\\n\")\n",
    "            f.write(f\"  Precision:    {metrics[f'{state}_precision']:.3f}\\n\")\n",
    "            f.write(f\"  Recall:       {metrics[f'{state}_recall']:.3f}\\n\")\n",
    "            f.write(f\"  F1 Score:     {metrics[f'{state}_f1']:.3f}\\n\")\n",
    "            f.write(f\"  Specificity:  {metrics[f'{state}_specificity']:.3f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# data_path = r\"C:\\Users\\joems\\OneDrive\\Desktop\\MLCV Project Items\\Machine Learning CS6140\\dataset\\CB513.npy\"\n",
    "# \"\"\"\n",
    "\n",
    "def load_and_validate_data(data_path: str) -> np.ndarray:\n",
    "    \"\"\"Load NPY data with Python 2/3 compatibility\"\"\"\n",
    "    try:\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            data = np.load(data_path)\n",
    "            if len(w) > 0 and issubclass(w[-1].category, UserWarning):\n",
    "                py3_path = data_path.replace('.npy', '_py3.npy'); np.save(py3_path, data); data = np.load(py3_path)\n",
    "                logger.info(f\"Converted and saved Python 3 format to {py3_path}\")\n",
    "        return data\n",
    "    except Exception as e: logger.error(f\"Data loading failed: {str(e)}\"); return None\n",
    "\n",
    "def preprocess_sequences(data: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"Process raw data into sequences and labels\"\"\"\n",
    "    sequences, labels = [], []; processor = ProteinFeatures(data)\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                features, label = processor.extract_features()\n",
    "                if features.shape[0] > 0: sequences.append(features); labels.append(label)\n",
    "            except StopIteration: break\n",
    "            except ValueError as e: logger.warning(str(e)); continue\n",
    "    except Exception as e: logger.error(f\"Sequence processing failed: {str(e)}\")\n",
    "    logger.info(f\"Successfully processed {len(sequences)} sequences\")\n",
    "    return sequences, labels\n",
    "\n",
    "\n",
    "def split_dataset(sequences: List[np.ndarray], labels: List[np.ndarray], val_size: float = 0.3, test_size: float = 0.5, random_state: int = 42) -> Tuple[List[np.ndarray], ...]:\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "    train_seqs, temp_seqs, train_labels, temp_labels = train_test_split(sequences, labels, test_size=val_size, random_state=random_state)\n",
    "    val_seqs, test_seqs, val_labels, test_labels = train_test_split(temp_seqs, temp_labels, test_size=test_size, random_state=random_state)\n",
    "    return train_seqs, val_seqs, test_seqs, train_labels, val_labels, test_labels\n",
    "\n",
    "def plot_training_metrics(history: Dict[str, List[float]], save_dir: str = 'results') -> None:\n",
    "    \"\"\"Plot and save training metrics visualization\"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot likelihoods\n",
    "    ax1.plot(history['train_ll'], label='Train', color='blue', alpha=0.7)\n",
    "    if history['val_ll']: ax1.plot(history['val_ll'], label='Validation', color='red', alpha=0.7)\n",
    "    ax1.set_title('Training Progress (Log Likelihood)'); ax1.set_xlabel('Iteration'); ax1.set_ylabel('Log Likelihood'); ax1.legend()\n",
    "    \n",
    "    # Plot state usage\n",
    "    state_usage = np.array(history['state_usage'])\n",
    "    ax2.plot(state_usage, alpha=0.7); ax2.set_title('State Usage Distribution')\n",
    "    ax2.set_xlabel('Iteration'); ax2.set_ylabel('Probability'); ax2.legend(['H', 'E', 'C'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/training_metrics.png\")\n",
    "    plt.close()\n",
    "\n",
    "def save_evaluation_results(metrics: Dict[str, float], history: Dict[str, List[float]], save_dir: str = 'results') -> None:\n",
    "    \"\"\"Save evaluation metrics and training history\"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    results = { 'final_metrics': metrics, 'training_history': {k: v for k, v in history.items() if isinstance(v, list)}\n",
    "    }\n",
    "    np.save(f\"{save_dir}/evaluation_results.npy\", results)\n",
    "\n",
    "\n",
    "def main() -> Tuple[MixtureGaussianHMM, Dict[str, float], Dict[str, List[float]]]:\n",
    "    \"\"\"Main execution pipeline with modular components\"\"\"\n",
    "    start_time = time.time(); logger.info(\"Starting protein structure prediction pipeline\")\n",
    "    \n",
    "    # Load and process data\n",
    "    data_path = r\"C:\\Users\\joems\\OneDrive\\Desktop\\MLCV Project Items\\Machine Learning CS6140\\dataset\\CB513.npy\"\n",
    "    data = load_and_validate_data(data_path)\n",
    "    \n",
    "    if data is None: return None, None, None\n",
    "    sequences, labels = process_sequences(data)\n",
    "    if not sequences: return None, None, None\n",
    "\n",
    "    # Split dataset with stratification\n",
    "    train_seqs, val_seqs, test_seqs, train_labels, val_labels, test_labels = split_dataset( sequences, labels, val_size=0.2, test_size=0.2 )\n",
    "    \n",
    "    # Initialize configuration and model\n",
    "    config = ModelConfig()  # Use default initialization\n",
    "    # Optional: Modify config parameters if needed\n",
    "    config.learning_rate = 0.001\n",
    "    config.lr_decay = 0.98\n",
    "    config.min_std = 0.1\n",
    "    config.max_std = 2.0\n",
    "    \n",
    "    model = MixtureGaussianHMM(config);         \n",
    "    \n",
    "    # Add verification prints here\n",
    "    print(\"State priors:\", model.state_priors); print(\"Transition matrix shape:\", model.transitions.shape);\n",
    "    print(\"Emission means shape:\", model.emission_means.shape);  print(\"Mixture weights shape:\", model.mixture_weights.shape);\n",
    "    logger.info(\"Starting model training...\");\n",
    "    \n",
    "    history = model.train( train_seqs, val_sequences=val_seqs, val_labels=val_labels,\n",
    "        n_iterations=100, tolerance=1e-4 )\n",
    "    \n",
    "    # Evaluate and save results\n",
    "    test_metrics = evaluate_detailed(model, test_seqs, test_labels)\n",
    "    plot_training_metrics(history)\n",
    "    save_evaluation_results(test_metrics, history)\n",
    "    \n",
    "    # Log results\n",
    "    logger.info(\"\\nFinal Results:\")\n",
    "    logger.info(f\"Test Accuracy: {test_metrics['accuracy']:.3f}\")\n",
    "    for state in ['Helix', 'Sheet', 'Coil']:\n",
    "        logger.info(f\"{state} Metrics:\")\n",
    "        logger.info(f\"  F1: {test_metrics[f'{state}_f1']:.3f}\")\n",
    "        logger.info(f\"  Precision: {test_metrics[f'{state}_precision']:.3f}\")\n",
    "        logger.info(f\"  Recall: {test_metrics[f'{state}_recall']:.3f}\")\n",
    "\n",
    "    # Log final results\n",
    "    logger.info(\"\\nTraining Results:\")\n",
    "    logger.info(f\"Final Train LL: {history['train_ll'][-1]:.2f}\")\n",
    "    if history['val_ll']:\n",
    "        logger.info(f\"Final Val LL: {history['val_ll'][-1]:.2f}\")\n",
    "    logger.info(f\"Test Accuracy: {test_metrics['accuracy']:.3f}\")\n",
    "    \n",
    "    logger.info(f\"Per-state metrics:\")\n",
    "    state_names = ['Helix', 'Sheet', 'Coil']\n",
    "    for name in state_names:\n",
    "        logger.info(f\"{name} Metrics:\")\n",
    "        logger.info(f\"  Precision: {test_metrics[f'{name}_precision']:.3f}\")\n",
    "        logger.info(f\"  Recall: {test_metrics[f'{name}_recall']:.3f}\")\n",
    "\n",
    "    \n",
    "    # Save model if performance is good enough\n",
    "    if test_metrics['accuracy'] > 0.5:  # Adjust threshold as needed\n",
    "        model_path = f\"models/hmm_model_{time.strftime('%Y%m%d_%H%M')}.joblib\"\n",
    "        model.save_model(model_path)\n",
    "        logger.info(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model, test_metrics, history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, metrics, history = main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "09:38:39 | INFO | Starting protein structure prediction pipeline\n",
    "09:38:40 | INFO | Converted and saved Python 3 format to C:\\Users\\joems\\OneDrive\\Desktop\\MLCV Project Items\\Machine Learning CS6140\\dataset\\CB513_py3.npy\n",
    "09:38:40 | INFO | Initializing feature extraction for sequence shape: (514, 39900)\n",
    "09:38:40 | INFO | Successfully processed 514 sequences\n",
    "09:38:40 | WARNING | Found near-zero probabilities in transition matrix. Adjusting...\n",
    "09:38:40 | INFO | Initialized HMM with 3 states and 3 mixtures per state\n",
    "09:38:40 | INFO | Starting model training...\n",
    "09:38:40 | INFO | Starting training with 411 sequences\n",
    "09:38:40 | INFO | Found 411 valid sequences out of 411\n",
    "09:38:40 | INFO | \n",
    "Initial Model State:\n",
    "09:38:40 | INFO | Transition probabilities:\n",
    "[[9.00981089e-01 9.90089109e-02 9.99990000e-06]\n",
    " [1.00000000e-01 6.70000000e-01 2.30000000e-01]\n",
    " [1.00000000e-01 5.10000000e-01 3.90000000e-01]]\n",
    "09:38:40 | INFO | Mixture weights:\n",
    "[[0.46 0.35 0.19]\n",
    " [0.46 0.35 0.19]\n",
    " [0.46 0.35 0.19]]\n",
    "09:38:40 | INFO | State priors:\n",
    "[0.492 0.162 0.346]\n",
    "09:38:40 | INFO | \n",
    "First sequence details:\n",
    "09:38:40 | INFO | Valid positions: 69\n",
    "09:38:40 | WARNING | State probability collapse detected:\n",
    "09:38:40 | WARNING | State distributions: ['0.000013', '0.011012', '0.988836']\n",
    "09:38:40 | WARNING | Min allowed prob: 0.016\n",
    "09:38:40 | WARNING | Max allowed prob: 0.047\n",
    "09:38:40 | INFO | Initial emission probability ranges:\n",
    "09:38:40 | INFO | Min: 0.000000, Max: 0.999860\n",
    "09:38:40 | INFO | Mean per state: [1.29035766e-05 1.10123848e-02 9.88836291e-01]\n",
    "09:38:40 | INFO | Found 82 valid sequences out of 82\n",
    "State priors: [0.492 0.162 0.346]\n",
    "Transition matrix shape: (3, 3)\n",
    "Emission means shape: (3, 3, 46)\n",
    "Mixture weights shape: (3, 3)\n",
    "09:38:42 | WARNING | State probability collapse detected:\n",
    "09:38:42 | WARNING | State distributions: ['0.000000', '0.000037', '0.999823']\n",
    "09:38:42 | WARNING | Min allowed prob: 0.016\n",
    "09:38:42 | WARNING | Max allowed prob: 0.047\n",
    "09:38:44 | WARNING | State probability collapse detected:\n",
    "09:38:44 | WARNING | State distributions: ['0.000000', '0.000048', '0.999812']\n",
    "09:38:44 | WARNING | Min allowed prob: 0.016\n",
    "09:38:44 | WARNING | Max allowed prob: 0.047\n",
    "09:38:46 | WARNING | State probability collapse detected:\n",
    "09:38:46 | WARNING | State distributions: ['0.000002', '0.042266', '0.957599']\n",
    "09:38:46 | WARNING | Min allowed prob: 0.016\n",
    "09:38:46 | WARNING | Max allowed prob: 0.047\n",
    "09:38:48 | WARNING | State probability collapse detected:\n",
    "09:38:48 | WARNING | State distributions: ['0.000002', '0.033224', '0.966639']\n",
    "09:38:48 | WARNING | Min allowed prob: 0.016\n",
    "09:38:48 | WARNING | Max allowed prob: 0.047\n",
    "09:38:50 | WARNING | State probability collapse detected:\n",
    "09:38:50 | WARNING | State distributions: ['0.000063', '0.091233', '0.908577']\n",
    "09:38:50 | WARNING | Min allowed prob: 0.016\n",
    "09:38:50 | WARNING | Max allowed prob: 0.047\n",
    "09:38:52 | WARNING | State probability collapse detected:\n",
    "09:38:52 | WARNING | State distributions: ['0.000000', '0.000662', '0.999198']\n",
    "09:38:52 | WARNING | Min allowed prob: 0.016\n",
    "09:38:52 | WARNING | Max allowed prob: 0.047\n",
    "09:38:54 | WARNING | State probability collapse detected:\n",
    "09:38:54 | WARNING | State distributions: ['0.000000', '0.000001', '0.999859']\n",
    "09:38:54 | WARNING | Min allowed prob: 0.016\n",
    "09:38:54 | WARNING | Max allowed prob: 0.047\n",
    "09:38:56 | WARNING | State probability collapse detected:\n",
    "09:38:56 | WARNING | State distributions: ['0.000000', '0.000329', '0.999531']\n",
    "09:38:56 | WARNING | Min allowed prob: 0.016\n",
    "09:38:56 | WARNING | Max allowed prob: 0.047\n",
    "09:38:58 | WARNING | State probability collapse detected:\n",
    "09:38:58 | WARNING | State distributions: ['0.000002', '0.001236', '0.998623']\n",
    "09:38:58 | WARNING | Min allowed prob: 0.016\n",
    "09:38:58 | WARNING | Max allowed prob: 0.047\n",
    "09:39:00 | WARNING | State probability collapse detected:\n",
    "09:39:00 | WARNING | State distributions: ['0.000000', '0.003906', '0.995954']\n",
    "09:39:00 | WARNING | Min allowed prob: 0.016\n",
    "09:39:00 | WARNING | Max allowed prob: 0.047\n",
    "09:39:02 | WARNING | State probability collapse detected:\n",
    "09:39:02 | WARNING | State distributions: ['0.000000', '0.007894', '0.991967']\n",
    "09:39:02 | WARNING | Min allowed prob: 0.016\n",
    "09:39:02 | WARNING | Max allowed prob: 0.047\n",
    "09:39:04 | WARNING | State probability collapse detected:\n",
    "09:39:04 | WARNING | State distributions: ['0.000000', '0.008381', '0.991480']\n",
    "09:39:04 | WARNING | Min allowed prob: 0.016\n",
    "09:39:04 | WARNING | Max allowed prob: 0.047\n",
    "09:39:06 | WARNING | State probability collapse detected:\n",
    "09:39:06 | WARNING | State distributions: ['0.000000', '0.011795', '0.988066']\n",
    "09:39:06 | WARNING | Min allowed prob: 0.016\n",
    "09:39:06 | WARNING | Max allowed prob: 0.047\n",
    "09:39:08 | WARNING | State probability collapse detected:\n",
    "09:39:08 | WARNING | State distributions: ['0.000023', '0.018587', '0.981253']\n",
    "09:39:08 | WARNING | Min allowed prob: 0.016\n",
    "09:39:08 | WARNING | Max allowed prob: 0.047\n",
    "09:39:10 | WARNING | State probability collapse detected:\n",
    "09:39:10 | WARNING | State distributions: ['0.000000', '0.002495', '0.997365']\n",
    "09:39:10 | WARNING | Min allowed prob: 0.016\n",
    "09:39:10 | WARNING | Max allowed prob: 0.047\n",
    "09:39:12 | WARNING | State probability collapse detected:\n",
    "09:39:12 | WARNING | State distributions: ['0.000000', '0.000006', '0.999854']\n",
    "09:39:12 | WARNING | Min allowed prob: 0.016\n",
    "09:39:12 | WARNING | Max allowed prob: 0.047\n",
    "09:39:14 | WARNING | State probability collapse detected:\n",
    "09:39:14 | WARNING | State distributions: ['0.000000', '0.000033', '0.999827']\n",
    "09:39:14 | WARNING | Min allowed prob: 0.016\n",
    "09:39:14 | WARNING | Max allowed prob: 0.047\n",
    "09:39:16 | WARNING | State probability collapse detected:\n",
    "09:39:16 | WARNING | State distributions: ['0.000029', '0.016469', '0.983364']\n",
    "09:39:16 | WARNING | Min allowed prob: 0.016\n",
    "09:39:16 | WARNING | Max allowed prob: 0.047\n",
    "09:39:18 | WARNING | State probability collapse detected:\n",
    "09:39:18 | WARNING | State distributions: ['0.000000', '0.001579', '0.998281']\n",
    "09:39:18 | WARNING | Min allowed prob: 0.016\n",
    "09:39:18 | WARNING | Max allowed prob: 0.047\n",
    "09:39:20 | WARNING | State probability collapse detected:\n",
    "09:39:20 | WARNING | State distributions: ['0.000000', '0.000001', '0.999859']\n",
    "09:39:20 | WARNING | Min allowed prob: 0.016\n",
    "09:39:20 | WARNING | Max allowed prob: 0.047\n",
    "09:39:22 | WARNING | State probability collapse detected:\n",
    "09:39:22 | WARNING | State distributions: ['0.000000', '0.000778', '0.999082']\n",
    "09:39:22 | WARNING | Min allowed prob: 0.016\n",
    "09:39:22 | WARNING | Max allowed prob: 0.047\n",
    "09:39:24 | WARNING | State probability collapse detected:\n",
    "09:39:24 | WARNING | State distributions: ['0.000001', '0.001213', '0.998646']\n",
    "09:39:24 | WARNING | Min allowed prob: 0.016\n",
    "09:39:24 | WARNING | Max allowed prob: 0.047\n",
    "09:39:26 | WARNING | State probability collapse detected:\n",
    "09:39:26 | WARNING | State distributions: ['0.000000', '0.000001', '0.999859']\n",
    "09:39:26 | WARNING | Min allowed prob: 0.016\n",
    "09:39:26 | WARNING | Max allowed prob: 0.047\n",
    "09:39:28 | WARNING | State probability collapse detected:\n",
    "09:39:28 | WARNING | State distributions: ['0.000000', '0.006286', '0.993574']\n",
    "09:39:28 | WARNING | Min allowed prob: 0.016\n",
    "09:39:28 | WARNING | Max allowed prob: 0.047\n",
    "09:39:30 | WARNING | State probability collapse detected:\n",
    "09:39:30 | WARNING | State distributions: ['0.000001', '0.019100', '0.980762']\n",
    "09:39:30 | WARNING | Min allowed prob: 0.016\n",
    "09:39:30 | WARNING | Max allowed prob: 0.047\n",
    "09:39:32 | WARNING | State probability collapse detected:\n",
    "09:39:32 | WARNING | State distributions: ['0.000017', '0.010099', '0.989745']\n",
    "09:39:32 | WARNING | Min allowed prob: 0.016\n",
    "09:39:32 | WARNING | Max allowed prob: 0.047\n",
    "09:39:34 | WARNING | State probability collapse detected:\n",
    "09:39:34 | WARNING | State distributions: ['0.000005', '0.010922', '0.988934']\n",
    "09:39:34 | WARNING | Min allowed prob: 0.016\n",
    "09:39:34 | WARNING | Max allowed prob: 0.047\n",
    "09:39:36 | WARNING | State probability collapse detected:\n",
    "09:39:36 | WARNING | State distributions: ['0.000000', '0.000010', '0.999850']\n",
    "09:39:36 | WARNING | Min allowed prob: 0.016\n",
    "09:39:36 | WARNING | Max allowed prob: 0.047\n",
    "09:39:38 | WARNING | State probability collapse detected:\n",
    "09:39:38 | WARNING | State distributions: ['0.000000', '0.000166', '0.999694']\n",
    "09:39:38 | WARNING | Min allowed prob: 0.016\n",
    "09:39:38 | WARNING | Max allowed prob: 0.047\n",
    "09:39:40 | WARNING | State probability collapse detected:\n",
    "09:39:40 | WARNING | State distributions: ['0.000000', '0.000158', '0.999702']\n",
    "09:39:40 | WARNING | Min allowed prob: 0.016\n",
    "09:39:40 | WARNING | Max allowed prob: 0.047\n",
    "09:39:42 | WARNING | State probability collapse detected:\n",
    "09:39:42 | WARNING | State distributions: ['0.000001', '0.001568', '0.998292']\n",
    "09:39:42 | WARNING | Min allowed prob: 0.016\n",
    "09:39:42 | WARNING | Max allowed prob: 0.047\n",
    "09:39:44 | WARNING | State probability collapse detected:\n",
    "09:39:44 | WARNING | State distributions: ['0.000000', '0.001183', '0.998677']\n",
    "09:39:44 | WARNING | Min allowed prob: 0.016\n",
    "09:39:44 | WARNING | Max allowed prob: 0.047\n",
    "09:39:46 | WARNING | State probability collapse detected:\n",
    "09:39:46 | WARNING | State distributions: ['0.000000', '0.000565', '0.999295']\n",
    "09:39:46 | WARNING | Min allowed prob: 0.016\n",
    "09:39:46 | WARNING | Max allowed prob: 0.047\n",
    "09:39:48 | WARNING | State probability collapse detected:\n",
    "09:39:48 | WARNING | State distributions: ['0.000004', '0.038076', '0.961786']\n",
    "09:39:48 | WARNING | Min allowed prob: 0.016\n",
    "09:39:48 | WARNING | Max allowed prob: 0.047\n",
    "09:39:50 | WARNING | State probability collapse detected:\n",
    "09:39:50 | WARNING | State distributions: ['0.000001', '0.037711', '0.962153']\n",
    "09:39:50 | WARNING | Min allowed prob: 0.016\n",
    "09:39:50 | WARNING | Max allowed prob: 0.047\n",
    "09:39:52 | WARNING | State probability collapse detected:\n",
    "09:39:52 | WARNING | State distributions: ['0.000000', '0.000820', '0.999041']\n",
    "09:39:52 | WARNING | Min allowed prob: 0.016\n",
    "09:39:52 | WARNING | Max allowed prob: 0.047\n",
    "09:39:54 | WARNING | State probability collapse detected:\n",
    "09:39:54 | WARNING | State distributions: ['0.000000', '0.004230', '0.995630']\n",
    "09:39:54 | WARNING | Min allowed prob: 0.016\n",
    "09:39:54 | WARNING | Max allowed prob: 0.047\n",
    "09:39:56 | WARNING | State probability collapse detected:\n",
    "09:39:56 | WARNING | State distributions: ['0.000000', '0.002260', '0.997600']\n",
    "09:39:56 | WARNING | Min allowed prob: 0.016\n",
    "09:39:56 | WARNING | Max allowed prob: 0.047\n",
    "09:39:58 | WARNING | State probability collapse detected:\n",
    "09:39:58 | WARNING | State distributions: ['0.000002', '0.001384', '0.998474']\n",
    "09:39:58 | WARNING | Min allowed prob: 0.016\n",
    "09:39:58 | WARNING | Max allowed prob: 0.047\n",
    "09:40:00 | WARNING | State probability collapse detected:\n",
    "09:40:00 | WARNING | State distributions: ['0.000000', '0.000166', '0.999694']\n",
    "09:40:00 | WARNING | Min allowed prob: 0.016\n",
    "09:40:00 | WARNING | Max allowed prob: 0.047\n",
    "09:40:02 | WARNING | State probability collapse detected:\n",
    "09:40:02 | WARNING | State distributions: ['0.000000', '0.000031', '0.999829']\n",
    "09:40:02 | WARNING | Min allowed prob: 0.016\n",
    "09:40:02 | WARNING | Max allowed prob: 0.047\n",
    "09:40:04 | WARNING | State probability collapse detected:\n",
    "09:40:04 | WARNING | State distributions: ['0.000000', '0.002492', '0.997368']\n",
    "09:40:04 | WARNING | Min allowed prob: 0.016\n",
    "09:40:04 | WARNING | Max allowed prob: 0.047\n",
    "09:40:06 | WARNING | State probability collapse detected:\n",
    "09:40:06 | WARNING | State distributions: ['0.000000', '0.000046', '0.999814']\n",
    "09:40:06 | WARNING | Min allowed prob: 0.016\n",
    "09:40:06 | WARNING | Max allowed prob: 0.047\n",
    "09:40:08 | WARNING | State probability collapse detected:\n",
    "09:40:08 | WARNING | State distributions: ['0.000000', '0.000032', '0.999828']\n",
    "09:40:08 | WARNING | Min allowed prob: 0.016\n",
    "09:40:08 | WARNING | Max allowed prob: 0.047\n",
    "09:40:10 | WARNING | State probability collapse detected:\n",
    "09:40:10 | WARNING | State distributions: ['0.000000', '0.000000', '0.999860']\n",
    "09:40:10 | WARNING | Min allowed prob: 0.016\n",
    "09:40:10 | WARNING | Max allowed prob: 0.047\n",
    "09:40:12 | WARNING | State probability collapse detected:\n",
    "09:40:12 | WARNING | State distributions: ['0.000000', '0.008973', '0.990888']\n",
    "09:40:12 | WARNING | Min allowed prob: 0.016\n",
    "09:40:12 | WARNING | Max allowed prob: 0.047\n",
    "09:40:14 | WARNING | State probability collapse detected:\n",
    "09:40:14 | WARNING | State distributions: ['0.000000', '0.000329', '0.999531']\n",
    "09:40:14 | WARNING | Min allowed prob: 0.016\n",
    "09:40:14 | WARNING | Max allowed prob: 0.047\n",
    "09:40:16 | WARNING | State probability collapse detected:\n",
    "09:40:16 | WARNING | State distributions: ['0.000000', '0.001816', '0.998044']\n",
    "09:40:16 | WARNING | Min allowed prob: 0.016\n",
    "09:40:16 | WARNING | Max allowed prob: 0.047\n",
    "09:40:18 | WARNING | State probability collapse detected:\n",
    "09:40:18 | WARNING | State distributions: ['0.000002', '0.001263', '0.998595']\n",
    "09:40:18 | WARNING | Min allowed prob: 0.016\n",
    "09:40:18 | WARNING | Max allowed prob: 0.047\n",
    "09:40:20 | WARNING | State probability collapse detected:\n",
    "09:40:20 | WARNING | State distributions: ['0.000000', '0.001154', '0.998707']\n",
    "09:40:20 | WARNING | Min allowed prob: 0.016\n",
    "09:40:20 | WARNING | Max allowed prob: 0.047\n",
    "09:40:22 | WARNING | State probability collapse detected:\n",
    "09:40:22 | WARNING | State distributions: ['0.000001', '0.003030', '0.996829']\n",
    "09:40:22 | WARNING | Min allowed prob: 0.016\n",
    "09:40:22 | WARNING | Max allowed prob: 0.047\n",
    "09:40:24 | WARNING | State probability collapse detected:\n",
    "09:40:24 | WARNING | State distributions: ['0.000133', '0.002382', '0.997345']\n",
    "09:40:24 | WARNING | Min allowed prob: 0.016\n",
    "09:40:24 | WARNING | Max allowed prob: 0.047\n",
    "09:40:26 | WARNING | State probability collapse detected:\n",
    "09:40:26 | WARNING | State distributions: ['0.000002', '0.001030', '0.998828']\n",
    "09:40:26 | WARNING | Min allowed prob: 0.016\n",
    "09:40:26 | WARNING | Max allowed prob: 0.047\n",
    "09:40:28 | WARNING | State probability collapse detected:\n",
    "09:40:28 | WARNING | State distributions: ['0.000000', '0.000753', '0.999108']\n",
    "09:40:28 | WARNING | Min allowed prob: 0.016\n",
    "09:40:28 | WARNING | Max allowed prob: 0.047\n",
    "09:40:30 | WARNING | State probability collapse detected:\n",
    "09:40:30 | WARNING | State distributions: ['0.000000', '0.001735', '0.998124']\n",
    "09:40:30 | WARNING | Min allowed prob: 0.016\n",
    "09:40:30 | WARNING | Max allowed prob: 0.047\n",
    "09:40:32 | WARNING | State probability collapse detected:\n",
    "09:40:32 | WARNING | State distributions: ['0.000000', '0.000268', '0.999592']\n",
    "09:40:32 | WARNING | Min allowed prob: 0.016\n",
    "09:40:32 | WARNING | Max allowed prob: 0.047\n",
    "09:40:34 | WARNING | State probability collapse detected:\n",
    "09:40:34 | WARNING | State distributions: ['0.000000', '0.004259', '0.995602']\n",
    "09:40:34 | WARNING | Min allowed prob: 0.016\n",
    "09:40:34 | WARNING | Max allowed prob: 0.047\n",
    "09:40:36 | WARNING | State probability collapse detected:\n",
    "09:40:36 | WARNING | State distributions: ['0.000000', '0.000653', '0.999207']\n",
    "09:40:36 | WARNING | Min allowed prob: 0.016\n",
    "09:40:36 | WARNING | Max allowed prob: 0.047\n",
    "09:40:38 | WARNING | State probability collapse detected:\n",
    "09:40:38 | WARNING | State distributions: ['0.000000', '0.002638', '0.997222']\n",
    "09:40:38 | WARNING | Min allowed prob: 0.016\n",
    "09:40:38 | WARNING | Max allowed prob: 0.047\n",
    "09:40:40 | WARNING | State probability collapse detected:\n",
    "09:40:40 | WARNING | State distributions: ['0.000000', '0.000182', '0.999678']\n",
    "09:40:40 | WARNING | Min allowed prob: 0.016\n",
    "09:40:40 | WARNING | Max allowed prob: 0.047\n",
    "09:40:42 | WARNING | State probability collapse detected:\n",
    "09:40:42 | WARNING | State distributions: ['0.000001', '0.000778', '0.999082']\n",
    "09:40:42 | WARNING | Min allowed prob: 0.016\n",
    "09:40:42 | WARNING | Max allowed prob: 0.047\n",
    "09:40:44 | WARNING | State probability collapse detected:\n",
    "09:40:44 | WARNING | State distributions: ['0.000000', '0.005545', '0.994316']\n",
    "09:40:44 | WARNING | Min allowed prob: 0.016\n",
    "09:40:44 | WARNING | Max allowed prob: 0.047\n",
    "09:40:46 | WARNING | State probability collapse detected:\n",
    "09:40:46 | WARNING | State distributions: ['0.000000', '0.007597', '0.992263']\n",
    "09:40:46 | WARNING | Min allowed prob: 0.016\n",
    "09:40:46 | WARNING | Max allowed prob: 0.047\n",
    "09:40:48 | WARNING | State probability collapse detected:\n",
    "09:40:48 | WARNING | State distributions: ['0.000000', '0.004769', '0.995092']\n",
    "09:40:48 | WARNING | Min allowed prob: 0.016\n",
    "09:40:48 | WARNING | Max allowed prob: 0.047\n",
    "09:40:50 | WARNING | State probability collapse detected:\n",
    "09:40:50 | WARNING | State distributions: ['0.000000', '0.000369', '0.999491']\n",
    "09:40:50 | WARNING | Min allowed prob: 0.016\n",
    "09:40:50 | WARNING | Max allowed prob: 0.047\n",
    "09:40:52 | WARNING | State probability collapse detected:\n",
    "09:40:52 | WARNING | State distributions: ['0.000000', '0.000689', '0.999171']\n",
    "09:40:52 | WARNING | Min allowed prob: 0.016\n",
    "09:40:52 | WARNING | Max allowed prob: 0.047\n",
    "09:40:54 | WARNING | State probability collapse detected:\n",
    "09:40:54 | WARNING | State distributions: ['0.000000', '0.000993', '0.998868']\n",
    "09:40:54 | WARNING | Min allowed prob: 0.016\n",
    "09:40:54 | WARNING | Max allowed prob: 0.047\n",
    "09:40:56 | WARNING | State probability collapse detected:\n",
    "09:40:56 | WARNING | State distributions: ['0.000000', '0.023470', '0.976394']\n",
    "09:40:56 | WARNING | Min allowed prob: 0.016\n",
    "09:40:56 | WARNING | Max allowed prob: 0.047\n",
    "09:40:58 | WARNING | State probability collapse detected:\n",
    "09:40:58 | WARNING | State distributions: ['0.000000', '0.014153', '0.985708']\n",
    "09:40:58 | WARNING | Min allowed prob: 0.016\n",
    "09:40:58 | WARNING | Max allowed prob: 0.047\n",
    "09:41:00 | WARNING | State probability collapse detected:\n",
    "09:41:00 | WARNING | State distributions: ['0.000000', '0.001030', '0.998830']\n",
    "09:41:00 | WARNING | Min allowed prob: 0.016\n",
    "09:41:00 | WARNING | Max allowed prob: 0.047\n",
    "09:41:02 | WARNING | State probability collapse detected:\n",
    "09:41:02 | WARNING | State distributions: ['0.000000', '0.002507', '0.997353']\n",
    "09:41:02 | WARNING | Min allowed prob: 0.016\n",
    "09:41:02 | WARNING | Max allowed prob: 0.047\n",
    "09:41:04 | WARNING | State probability collapse detected:\n",
    "09:41:04 | WARNING | State distributions: ['0.000000', '0.000031', '0.999829']\n",
    "09:41:04 | WARNING | Min allowed prob: 0.016\n",
    "09:41:04 | WARNING | Max allowed prob: 0.047\n",
    "---------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (protein_ss_prediction)",
   "language": "python",
   "name": "protein_ss_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
