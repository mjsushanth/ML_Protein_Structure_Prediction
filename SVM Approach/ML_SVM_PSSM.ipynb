{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEI6JXsWMuVK",
        "outputId": "7c1772d2-92aa-4acf-f656-22e923256e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWS2LzevL-U7",
        "outputId": "1cb60a8c-8f84-46db-ad91-bf87236a1b8d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading data...\n",
            "Raw data shape: (514, 39900)\n",
            "Features shape: (514, 700, 21)\n",
            "Labels shape: (514, 700)\n",
            "Class 0: 294744 (81.92%)\n",
            "Class 1: 47187 (13.11%)\n",
            "Class 2: 17869 (4.97%)\n",
            "Starting feature extraction and training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting features: 100%|██████████| 411/411 [01:09<00:00,  5.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction complete. Shape: (287700, 533)\n",
            "Starting model training...\n",
            "\n",
            "Training model for structure type Helix\n",
            "\n",
            "Epoch 0 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.7498\n",
            "Validation Accuracy: 0.6524\n",
            "\n",
            "Epoch 5 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.7498\n",
            "Validation Accuracy: 0.4534\n",
            "\n",
            "Epoch 10 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.7498\n",
            "Validation Accuracy: 0.4790\n",
            "\n",
            "Epoch 15 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.7498\n",
            "Validation Accuracy: 0.4772\n",
            "\n",
            "Training model for structure type Sheet\n",
            "\n",
            "Epoch 0 Summary\n",
            "------------------------------\n",
            "Training Loss: 1.0108\n",
            "Validation Accuracy: 0.9816\n",
            "\n",
            "Epoch 5 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.6217\n",
            "Validation Accuracy: 0.5147\n",
            "\n",
            "Epoch 10 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.6061\n",
            "Validation Accuracy: 0.5516\n",
            "\n",
            "Epoch 15 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.6373\n",
            "Validation Accuracy: 0.5540\n",
            "\n",
            "Training model for structure type Coil\n",
            "\n",
            "Epoch 0 Summary\n",
            "------------------------------\n",
            "Training Loss: 1.0352\n",
            "Validation Accuracy: 0.4766\n",
            "\n",
            "Epoch 5 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.6813\n",
            "Validation Accuracy: 0.4915\n",
            "\n",
            "Epoch 10 Summary\n",
            "------------------------------\n",
            "Training Loss: 1.0371\n",
            "Validation Accuracy: 0.5279\n",
            "\n",
            "Epoch 15 Summary\n",
            "------------------------------\n",
            "Training Loss: 0.6579\n",
            "Validation Accuracy: 0.4057\n",
            "\n",
            "Total training time: 737.43 seconds\n",
            "\n",
            "FINAL MODEL PERFORMANCE\n",
            "==================================================\n",
            "\n",
            "Overall Metrics:\n",
            "Average Accuracy: 0.7491\n",
            "Macro F1-Score: 0.7463\n",
            "\n",
            "Per-State Performance:\n",
            "\n",
            "============================================================\n",
            "State      Accuracy   Precision  Recall     F1-Score  \n",
            "------------------------------------------------------------\n",
            "Helix      0.9188     0.7401     0.9188     0.8198    \n",
            "Sheet      0.7139     0.7329     0.7139     0.7233    \n",
            "Coil       0.6253     0.7843     0.6253     0.6958    \n",
            "============================================================\n",
            "\n",
            "\n",
            "Generating predictions...\n",
            "\n",
            "============================================================\n",
            "State      Accuracy   Precision  Recall     F1-Score  \n",
            "------------------------------------------------------------\n",
            "Helix      0.9352     0.9748     0.9352     0.9546    \n",
            "Sheet      0.6277     0.7723     0.6277     0.6926    \n",
            "Coil       0.7248     0.3321     0.7248     0.4555    \n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from Bio.Align import substitution_matrices\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import time\n",
        "import os\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class IndependentPSSMGenerator:\n",
        "    def __init__(self, window_size=3):\n",
        "        self.window_size = window_size\n",
        "        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "        # implementing BLOSUM62 matrix directly\n",
        "        self.blosum62_dict = {\n",
        "            ('A', 'A'): 4, ('R', 'R'): 5, ('N', 'N'): 6, ('D', 'D'): 6, ('C', 'C'): 9,\n",
        "            ('Q', 'Q'): 5, ('E', 'E'): 5, ('G', 'G'): 6, ('H', 'H'): 8, ('I', 'I'): 4,\n",
        "            ('L', 'L'): 4, ('K', 'K'): 5, ('M', 'M'): 5, ('F', 'F'): 6, ('P', 'P'): 7,\n",
        "            ('S', 'S'): 4, ('T', 'T'): 5, ('W', 'W'): 11, ('Y', 'Y'): 7, ('V', 'V'): 4,\n",
        "            ('A', 'R'): -1, ('A', 'N'): -2, ('A', 'D'): -2, ('A', 'C'): 0, ('A', 'Q'): -1,\n",
        "            ('A', 'E'): -1, ('A', 'G'): 0, ('A', 'H'): -2, ('A', 'I'): -1, ('A', 'L'): -1,\n",
        "            ('A', 'K'): -1, ('A', 'M'): -1, ('A', 'F'): -2, ('A', 'P'): -1, ('A', 'S'): 1,\n",
        "            ('A', 'T'): 0, ('A', 'W'): -3, ('A', 'Y'): -2, ('A', 'V'): 0, ('R', 'N'): 0,\n",
        "        }\n",
        "        self.blosum_matrix = self._creating_blosum_matrix()\n",
        "\n",
        "    def _creating_blosum_matrix(self):\n",
        "        n = len(self.amino_acids)\n",
        "        matrix = np.zeros((n, n), dtype=np.float32)\n",
        "        for i, aa1 in enumerate(self.amino_acids):\n",
        "            for j, aa2 in enumerate(self.amino_acids):\n",
        "                # Try both orientations of the pair\n",
        "                score = self.blosum62_dict.get((aa1, aa2)) or self.blosum62_dict.get((aa2, aa1))\n",
        "                if score is None:\n",
        "                    score = -4  # Default penalty for unknown pairs\n",
        "                matrix[i, j] = score\n",
        "        return matrix\n",
        "\n",
        "    def _calculating_frequency_profile(self, sequence_profile):\n",
        "        profile = sequence_profile.copy()\n",
        "        totals = np.sum(profile, axis=1, keepdims=True)\n",
        "        totals[totals == 0] = 1.0\n",
        "        return profile / totals\n",
        "\n",
        "    def _calculating_pssm_scores(self, freq_profile):\n",
        "        background_freq = np.ones(len(self.amino_acids)) / len(self.amino_acids)\n",
        "        pssm_scores = np.zeros_like(freq_profile)\n",
        "\n",
        "        for i in range(freq_profile.shape[0]):\n",
        "            for j in range(freq_profile.shape[1]):\n",
        "                if freq_profile[i, j] > 0:\n",
        "                    pssm_scores[i, j] = np.log2(freq_profile[i, j] / background_freq[j])\n",
        "                else:\n",
        "                    # giving penalty for zero frequency\n",
        "                    pssm_scores[i, j] = -5\n",
        "\n",
        "        return pssm_scores\n",
        "\n",
        "    def generating_pssm(self, sequence_features):\n",
        "        sequence_profile = sequence_features[:, :20].astype(np.float32)\n",
        "        freq_profile = self._calculating_frequency_profile(sequence_profile)\n",
        "\n",
        "        seq_len = len(sequence_profile)\n",
        "        pssm = np.zeros((seq_len, 20), dtype=np.float32)\n",
        "        conservation = np.zeros(seq_len, dtype=np.float32)\n",
        "\n",
        "        pad_size = self.window_size // 2\n",
        "        padded_profile = np.pad(freq_profile, ((pad_size, pad_size), (0, 0)))\n",
        "        epsilon = 1e-10\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            window = padded_profile[i:i+self.window_size]\n",
        "            pos_freq = np.mean(window, axis=0)\n",
        "\n",
        "            # calculating the position-specific scores\n",
        "            weighted_freq = np.dot(pos_freq, self.blosum_matrix)\n",
        "            weighted_freq = np.clip(weighted_freq, epsilon, None)\n",
        "            weighted_freq /= np.sum(weighted_freq)\n",
        "\n",
        "            # calculating the PSSM scores\n",
        "            pssm[i] = self._calculating_pssm_scores(weighted_freq.reshape(1, -1))[0]\n",
        "\n",
        "            # calculating the conservation\n",
        "            entropy = -np.sum(weighted_freq * np.log2(np.maximum(weighted_freq, epsilon)))\n",
        "            conservation[i] = 1 - (entropy / np.log2(20))\n",
        "\n",
        "        return pssm, conservation\n",
        "    def comparing_pssm_with_original_pssm(self, sequence_features, generated_pssm):\n",
        "        try:\n",
        "            if sequence_features is None or generated_pssm is None:\n",
        "                print(\"Invalid inputs for PSSM comparison\")\n",
        "                return {'correlation': 0, 'mse': 0}\n",
        "            original_pssm = sequence_features[:, 21:41]\n",
        "\n",
        "            if original_pssm.shape[1] != 20 or generated_pssm.shape[1] != 20:\n",
        "                print(f\"Invalid PSSM dimensions - Original: {original_pssm.shape}, Generated: {generated_pssm.shape}\")\n",
        "                return {'correlation': 0, 'mse': 0}\n",
        "\n",
        "            if original_pssm.shape[0] != generated_pssm.shape[0]:\n",
        "                min_len = min(original_pssm.shape[0], generated_pssm.shape[0])\n",
        "                original_pssm = original_pssm[:min_len]\n",
        "                generated_pssm = generated_pssm[:min_len]\n",
        "                print(f\"Truncated matrices to length {min_len}\")\n",
        "\n",
        "            original_pssm = (original_pssm - np.mean(original_pssm)) / np.std(original_pssm)\n",
        "            generated_pssm = (generated_pssm - np.mean(generated_pssm)) / np.std(generated_pssm)\n",
        "\n",
        "            correlation = np.corrcoef(original_pssm.flatten(), generated_pssm.flatten())[0,1]\n",
        "            mse = np.mean((original_pssm - generated_pssm) ** 2)\n",
        "\n",
        "            plt.figure(figsize=(15, 5))\n",
        "            plt.subplot(121)\n",
        "            sns.heatmap(original_pssm, cmap='coolwarm', center=0)\n",
        "            plt.title('Original PSSM')\n",
        "            plt.xlabel('Amino Acid Position')\n",
        "            plt.ylabel('Sequence Position')\n",
        "\n",
        "            plt.subplot(122)\n",
        "            sns.heatmap(generated_pssm, cmap='coolwarm', center=0)\n",
        "            plt.title('Generated PSSM')\n",
        "            plt.xlabel('Amino Acid Position')\n",
        "            plt.ylabel('Sequence Position')\n",
        "\n",
        "            plt.suptitle(f'PSSM Comparison\\nCorrelation: {correlation:.3f}, MSE: {mse:.3f}')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            return {\n",
        "                'correlation': correlation,\n",
        "                'mse': mse,\n",
        "                'shape': original_pssm.shape\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error in PSSM comparison: {str(e)}\")\n",
        "            print(traceback.format_exc())\n",
        "            return {'correlation': 0, 'mse': 0}\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_iters=100, class_weight=None):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.class_weight = class_weight\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features, dtype=np.float32)\n",
        "        y = np.where(y == 1, 1, -1)\n",
        "\n",
        "        # improving class weighting\n",
        "        if self.class_weight is not None:\n",
        "            pos_weight = self.class_weight\n",
        "            neg_weight = 1.0\n",
        "            sample_weights = np.where(y == 1, pos_weight, neg_weight)\n",
        "        else:\n",
        "            sample_weights = np.ones(n_samples)\n",
        "\n",
        "        # adjusting dynamic learning rate\n",
        "        adaptive_lr = self.lr\n",
        "\n",
        "        for iter in range(self.n_iters):\n",
        "            # shuffling data at each iteration\n",
        "            shuffle_idx = np.random.permutation(n_samples)\n",
        "            X = X[shuffle_idx]\n",
        "            y = y[shuffle_idx]\n",
        "            sample_weights = sample_weights[shuffle_idx]\n",
        "\n",
        "            margins = y * (np.dot(X, self.w) + self.b)\n",
        "            mask = margins < 1\n",
        "            # updates only if there are violations\n",
        "            if np.sum(mask) > 0:\n",
        "                weighted_grad = np.sum(X[mask] * y[mask, np.newaxis] * sample_weights[mask, np.newaxis], axis=0)\n",
        "\n",
        "                # applying L2 regularization with momentum\n",
        "                self.w = self.w * (1 - self.lambda_param * adaptive_lr) + adaptive_lr * weighted_grad\n",
        "                self.b += adaptive_lr * np.sum(y[mask] * sample_weights[mask])\n",
        "\n",
        "                # adjusting the learning rate based on the margin violations\n",
        "                violation_rate = np.mean(mask)\n",
        "                adaptive_lr = self.lr * (1.0 / (1.0 + violation_rate))\n",
        "\n",
        "    def predict(self, X):\n",
        "        # adding the confidence scores to the predictions\n",
        "        raw_predictions = np.dot(X, self.w) + self.b\n",
        "        confidence = np.abs(raw_predictions)\n",
        "\n",
        "        # applying the class weights to predictions\n",
        "        if self.class_weight is not None:\n",
        "            raw_predictions *= np.where(raw_predictions > 0, self.class_weight, 1.0)\n",
        "\n",
        "        return np.sign(raw_predictions), confidence\n",
        "\n",
        "def calculating_class_weights(labels):\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    total = len(labels)\n",
        "    weights = {i: total/(len(unique) * count) for i, count in zip(unique, counts)}\n",
        "    return weights\n",
        "\n",
        "def display_metrics_table(metrics_dict):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'State':<10} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
        "    print(\"-\"*60)\n",
        "    for state, vals in metrics_dict.items():\n",
        "        print(f\"{state:<10} {vals['accuracy']:<10.4f} {vals['precision']:<10.4f} {vals['recall']:<10.4f} {vals['f1']:<10.4f}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "def display_epoch_summary(epoch, train_loss, val_acc):\n",
        "    print(f\"\\nEpoch {epoch} Summary\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "def display_final_results(results):\n",
        "    print(\"\\nFINAL MODEL PERFORMANCE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    print(f\"Average Accuracy: {results['overall_acc']:.4f}\")\n",
        "    print(f\"Macro F1-Score: {results['macro_f1']:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-State Performance:\")\n",
        "    display_metrics_table(results['state_metrics'])\n",
        "\n",
        "def plotting_training_metrics(metrics):\n",
        "    if not all(metrics.values()):\n",
        "        print(\"No metrics to plot yet\")\n",
        "        return\n",
        "\n",
        "    # getting the lengths for each metric\n",
        "    lengths = {k: len(v) for k,v in metrics.items() if v}\n",
        "    min_len = min(lengths.values())\n",
        "\n",
        "    epochs = range(1, min_len + 1)\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plt.plot(epochs, metrics['train_loss'][:min_len], 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, metrics['val_loss'][:min_len], 'r-', label='Validation Loss')\n",
        "    plt.title('Loss over Training')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(122)\n",
        "    for state in metrics['state_metrics'][-1].keys():\n",
        "        plt.plot(epochs,\n",
        "                 [m[state]['f1'] for m in metrics['state_metrics']],\n",
        "                 label = f'{state} F1')\n",
        "        plt.legend()\n",
        "def extracting_features_batch(sequence):\n",
        "    return sequence\n",
        "class ProteinStructurePredictor:\n",
        "\n",
        "    def __init__(self, window_size=13):\n",
        "        self.window_size = window_size\n",
        "        self.pssm_generator = IndependentPSSMGenerator(window_size=3)\n",
        "        self.svm_models = []\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def extracting_features(self, sequence_features):\n",
        "        return self.optimizing_feature_extraction(sequence_features)\n",
        "    def _calculating_importance(self, y_train, indices, window_size=5):\n",
        "\n",
        "        importance = np.ones(len(indices), dtype=np.float32)\n",
        "        padded_y = np.pad(y_train, window_size//2, mode='edge')\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            # getting the window of labels\n",
        "            window = padded_y[idx:idx + window_size]\n",
        "\n",
        "            # calculating transition score\n",
        "            transitions = np.sum(np.abs(np.diff(window)))\n",
        "\n",
        "            # calculating minority presence\n",
        "            minority_count = np.sum(window != 0)  # non-helix states\n",
        "\n",
        "            # calculating position importance\n",
        "            boundary_distance = min(idx, len(y_train) - idx)\n",
        "            position_weight = 1.0 / (1.0 + np.exp(-boundary_distance/100))\n",
        "\n",
        "            # combining all the scores\n",
        "            importance[i] = (1 + transitions) * (1 + minority_count) * position_weight\n",
        "\n",
        "        return importance\n",
        "\n",
        "    def extract_balanced_batch(self, X, y, batch_size):\n",
        "        indices = self.weighted_sampling(y, batch_size)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "    def weighted_sampling(self, y_train, batch_size):\n",
        "        class_counts = np.bincount(y_train.astype(int))\n",
        "\n",
        "        base_ratios = {\n",
        "            0: 0.25,\n",
        "            1: 0.40,\n",
        "            2: 0.35\n",
        "        }\n",
        "        batch_sizes = {i: max(int(batch_size * ratio), 50)\n",
        "                       for i, ratio in base_ratios.items()}\n",
        "        indices = []\n",
        "        for class_idx in range(3):\n",
        "            class_indices = np.where(y_train == class_idx)[0]\n",
        "            if len(class_indices) == 0:\n",
        "                continue\n",
        "            # calculating structural importance\n",
        "            importance = self._calculating_importance(\n",
        "                y_train, class_indices, window_size=5\n",
        "            )\n",
        "\n",
        "            if np.all(importance == 0):\n",
        "                importance = np.ones_like(importance)\n",
        "            # normalizing probabilities\n",
        "            p = importance / importance.sum()\n",
        "\n",
        "            selected_indices = np.random.choice(\n",
        "                class_indices,\n",
        "                size=batch_sizes[class_idx],\n",
        "                p=p,\n",
        "                replace=True\n",
        "            )\n",
        "            indices.extend(selected_indices)\n",
        "        return np.array(indices)\n",
        "\n",
        "    def validating_with_uncertainty(self, X_val, y_val):\n",
        "        results = {}\n",
        "        uncertainties = []\n",
        "        predictions = []\n",
        "\n",
        "        for i, model in enumerate(self.svm_models):\n",
        "            pred, conf = model.predict(X_val)\n",
        "            binary_val = (y_val == i).astype(np.float32)\n",
        "            uncertainty = 1 / (1 + np.exp(-conf))  # Transform confidence to uncertainty\n",
        "            uncertainties.append(uncertainty)\n",
        "\n",
        "            weights = 1 - uncertainty\n",
        "            weighted_acc = np.average(binary_val == (pred > 0), weights=weights)\n",
        "            results[f'class_{i}_accuracy'] = weighted_acc\n",
        "            results[f'class_{i}_uncertainty'] = np.mean(uncertainty)\n",
        "            results[f'class_{i}_conf_mean'] = np.mean(conf)\n",
        "\n",
        "            predictions.append((pred > 0).astype(int))\n",
        "\n",
        "        # Aggregate predictions for all classes\n",
        "        predictions = np.argmax(np.array(predictions), axis=0)\n",
        "        # Calculate overall accuracy\n",
        "        overall_acc = np.mean(predictions == y_val)\n",
        "        results['overall_acc'] = overall_acc\n",
        "        # Add state-wise metrics to results\n",
        "        state_metrics = self.validating_per_state(predictions, y_val)\n",
        "\n",
        "        results['state_metrics'] = state_metrics  # Add this line here\n",
        "        f1_scores = [state_metrics[state]['f1'] for state in state_metrics]\n",
        "\n",
        "        macro_f1 = np.mean(f1_scores)\n",
        "        results['macro_f1'] = macro_f1\n",
        "\n",
        "        results['predictions'] = predictions\n",
        "        return results, np.array(uncertainties)\n",
        "\n",
        "    def balance_dataset(self, X, y):\n",
        "        class_counts = np.bincount(y.astype(int))\n",
        "        target_count = int(0.7 * np.max(class_counts))  # 60% of majority class\n",
        "        balanced_X = []\n",
        "        balanced_y = []\n",
        "        for i in range(len(class_counts)):\n",
        "            idx = np.where(y == i)[0]\n",
        "            if len(idx) < target_count:\n",
        "                n_copies = int(np.ceil(target_count / len(idx)))\n",
        "                for _ in range(n_copies):\n",
        "                    sampled_X = X[idx] + np.random.normal(0, 0.01, X[idx].shape)\n",
        "                    balanced_X.extend(sampled_X)\n",
        "                    balanced_y.extend([i] * len(idx))\n",
        "            else:\n",
        "                selected_idx = np.random.choice(idx, target_count, replace=False)\n",
        "                balanced_X.extend(X[selected_idx])\n",
        "                balanced_y.extend([i] * len(selected_idx))\n",
        "        return np.array(balanced_X), np.array(balanced_y)\n",
        "\n",
        "    def optimizing_feature_extraction(self, sequence_features):\n",
        "        pssm, conservation = self.pssm_generator.generating_pssm(sequence_features)\n",
        "        pad_size = self.window_size // 2\n",
        "\n",
        "        padded_features = {\n",
        "            'pssm': np.pad(pssm, ((pad_size, pad_size), (0, 0)), constant_values=0),\n",
        "            'cons': np.pad(conservation, pad_size, constant_values=0),\n",
        "            'profile': np.pad(sequence_features[:, :20], ((pad_size, pad_size), (0, 0)), constant_values=0)\n",
        "        }\n",
        "        n_positions = len(sequence_features)\n",
        "        feature_size = self.window_size * (20 + 1 + 20)\n",
        "        # preallocating the output array\n",
        "        features = np.zeros((n_positions, feature_size), dtype=np.float32)# Vectorized feature extraction\n",
        "        for i in range(n_positions):\n",
        "            window_slice = slice(i, i + self.window_size)\n",
        "            features[i] = np.concatenate([\n",
        "                padded_features['pssm'][window_slice].ravel(),\n",
        "                padded_features['cons'][window_slice],\n",
        "                padded_features['profile'][window_slice].ravel()\n",
        "            ])\n",
        "        return features, pssm\n",
        "\n",
        "    def analyzing_epoch_distribution(self, y_true, y_pred, epoch):\n",
        "        states = ['Helix', 'Sheet', 'Coil']\n",
        "        distributions = {}\n",
        "\n",
        "        for i, state in enumerate(states):\n",
        "            state_mask = (y_true == i)\n",
        "            tp = np.sum((y_true == i) & (y_pred == i))\n",
        "            fp = np.sum((y_true != i) & (y_pred == i))\n",
        "            fn = np.sum((y_true == i) & (y_pred != i))\n",
        "\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "            distributions[f'{state}_precision'] = precision\n",
        "            distributions[f'{state}_recall'] = recall\n",
        "\n",
        "            # tracking transitions\n",
        "            for j, other_state in enumerate(states):\n",
        "                if i != j:\n",
        "                    transitions = np.sum((y_true == i) & (y_pred == j))\n",
        "                    distributions[f'{state}_to_{other_state}'] = transitions\n",
        "\n",
        "\n",
        "        return distributions\n",
        "    def tracking_pssm_quality(self, sequence_features, generated_pssm, epoch):\n",
        "        try:\n",
        "            metrics = self.pssm_generator.comparing_pssm_with_original_pssm(sequence_features, generated_pssm)\n",
        "            if metrics['correlation'] != 0:\n",
        "                return metrics\n",
        "        except Exception as e:\n",
        "            print(f\"Error tracking PSSM Quality: {e}\")\n",
        "            return {'correlation':0, 'mse':0}\n",
        "    def plotting_pssm_progress(self, metrics):\n",
        "        try:\n",
        "            epochs = sorted([k.split('_')[1] for k in metrics.keys() if k.startswith('epoch_')])\n",
        "            correlations = [metrics[f'epoch_{e}_pssm_seq_0']['correlation'] for e in epochs]\n",
        "            mses = [metrics[f'epoch_{e}_pssm_seq_0']['mse'] for e in epochs]\n",
        "\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.subplot(121)\n",
        "            plt.plot(epochs, correlations, 'b-')\n",
        "            plt.title('PSSM Correlation over Epochs')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Correlation')\n",
        "\n",
        "            plt.subplot(122)\n",
        "            plt.plot(epochs, mses, 'r-')\n",
        "            plt.title('PSSM MSE over Epochs')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MSE')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting PSSM progress: {e}\")\n",
        "\n",
        "    def checking_convergence(self, metrics_history, window=7, threshold=0.003):\n",
        "        if len(metrics_history) < window:\n",
        "            return False\n",
        "        recent_metrics = metrics_history[-window:]\n",
        "        # checking both the stability and minimum performance\n",
        "        stable = all(np.std(m) < threshold for m in zip(*recent_metrics))\n",
        "        min_f1 = min(min(metrics) for metrics in recent_metrics)\n",
        "        # Only converge if we have both stability and good performance\n",
        "        return stable and min_f1 > 0.4\n",
        "\n",
        "    def balance_dataset(self, X, y):\n",
        "\n",
        "        # calculating the structural importance scores\n",
        "        importance_scores = self._calculating_structural_scores(X, y)\n",
        "\n",
        "        class_counts = np.bincount(y.astype(int))\n",
        "        target_count = int(0.7 * np.max(class_counts))\n",
        "\n",
        "        balanced_X = []\n",
        "        balanced_y = []\n",
        "\n",
        "        for i in range(len(class_counts)):\n",
        "            idx = np.where(y == i)[0]\n",
        "\n",
        "            if len(idx) < target_count:\n",
        "                n_copies = int(np.ceil(target_count / len(idx)))\n",
        "                for _ in range(n_copies):\n",
        "                    sampled_X = X[idx] + np.random.normal(0, 0.01, X[idx].shape)\n",
        "                    balanced_X.extend(sampled_X)\n",
        "                    balanced_y.extend([i] * len(idx))\n",
        "            else:\n",
        "                # adding the importance-based selection for majority class\n",
        "                scores = importance_scores[idx]\n",
        "                selected_idx = self._selecting_important_samples(\n",
        "                    idx, scores, target_count\n",
        "                )\n",
        "                balanced_X.extend(X[selected_idx])\n",
        "                balanced_y.extend([i] * len(selected_idx))\n",
        "\n",
        "        return np.array(balanced_X), np.array(balanced_y)\n",
        "\n",
        "    def _calculating_structural_scores(self, X, y, window=5):\n",
        "        scores = np.zeros(len(y))\n",
        "        padded_y = np.pad(y, (window//2, window//2), mode='edge')\n",
        "\n",
        "        for i in range(len(y)):\n",
        "            window_labels = padded_y[i:i+window]\n",
        "            transitions = np.sum(np.abs(np.diff(window_labels)))\n",
        "            minority_presence = np.sum(window_labels > 0)\n",
        "            scores[i] = transitions + 0.5 * minority_presence\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def _selecting_important_samples(self, indices, scores, target_count):\n",
        "        if len(indices) <= target_count:\n",
        "            return indices\n",
        "\n",
        "        # combining the random selection with importance based selection\n",
        "        n_important = int(target_count * 0.7)\n",
        "        n_random = target_count - n_important\n",
        "\n",
        "        # selecting the important samples\n",
        "        important_idx = indices[np.argsort(scores)[-n_important:]]\n",
        "\n",
        "        # selecting the random samples from the remaining\n",
        "        remaining_idx = np.setdiff1d(indices, important_idx)\n",
        "        random_idx = np.random.choice(\n",
        "            remaining_idx,\n",
        "            size=min(n_random, len(remaining_idx)),\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        return np.concatenate([important_idx, random_idx])\n",
        "\n",
        "    def train(self, sequences, labels):\n",
        "        start_time = time.time()\n",
        "        print(\"Extracting features and training...\")\n",
        "\n",
        "        metrics = {\n",
        "            'train_loss': [], 'val_loss': [], 'accuracy': [],\n",
        "            'Helix_recall': [], 'Sheet_recall': [], 'Coil_recall': [],\n",
        "            'state_metrics': [], 'uncertainties': []\n",
        "        }\n",
        "        # using optimized feature extraction\n",
        "        n_processes = max(os.cpu_count() - 1, 1)\n",
        "        with Pool(processes=n_processes) as pool:\n",
        "            feature_results = list(tqdm(\n",
        "                pool.imap(self.optimizing_feature_extraction, sequences, chunksize=8),\n",
        "                total=len(sequences),\n",
        "                desc=\"Extracting features\"\n",
        "            ))\n",
        "        # combining all the features\n",
        "        all_features = [f[0] for f in feature_results]\n",
        "        X = np.vstack(all_features).astype(np.float32)\n",
        "        y = np.concatenate([label.flatten() for label in labels]).astype(np.float32)\n",
        "        print(f\"Feature extraction complete. Shape: {X.shape}\")\n",
        "\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        # using enhanced balancing\n",
        "        X_balanced, y_balanced = self.balance_dataset(X_scaled, y)\n",
        "        # splitting after balancing\n",
        "        train_indices, val_indices = train_test_split(\n",
        "            np.arange(len(y_balanced)),\n",
        "            test_size=0.1,\n",
        "            stratify=y_balanced\n",
        "        )\n",
        "        X_train = X_balanced[train_indices]\n",
        "        y_train = y_balanced[train_indices]\n",
        "        X_val = X_balanced[val_indices]\n",
        "        y_val = y_balanced[val_indices]\n",
        "\n",
        "        # training with the improved sampling\n",
        "        print(\"training model..\")\n",
        "        self.svm_models = []\n",
        "        n_epochs = 20\n",
        "        batch_size = 1024\n",
        "        state_metrics_history = []\n",
        "\n",
        "        for i in range(3):\n",
        "            print(f\"\\nTraining model for structure type {['Helix', 'Sheet', 'Coil'][i]}\")\n",
        "            model = SVM(\n",
        "                learning_rate=0.01,\n",
        "                lambda_param=0.001,\n",
        "                n_iters=200,\n",
        "                # handling class weights through sampling\n",
        "                class_weight=None\n",
        "            )\n",
        "            self.svm_models.append(model)\n",
        "            for epoch in range(n_epochs):\n",
        "                X_batch, y_batch = self.extract_balanced_batch(X_train, y_train, batch_size)\n",
        "                y_batch = (y_batch == i).astype(np.float32)\n",
        "                # training on the batch\n",
        "                model.fit(X_batch, y_batch)\n",
        "\n",
        "                # evaluating every 5 epochs\n",
        "                if epoch % 5 == 0:\n",
        "                    val_results, uncertainties = self.validating_with_uncertainty(X_val, y_val)\n",
        "                    metrics['uncertainties'].append(uncertainties)\n",
        "\n",
        "                    # updating the metrics\n",
        "                    state_metrics = self.validating_per_state(\n",
        "                        (val_results['predictions']).astype(int),\n",
        "                        y_val.astype(int)\n",
        "                    )\n",
        "                    metrics['state_metrics'].append(state_metrics)\n",
        "\n",
        "                    # tracking the training progress\n",
        "                    train_pred, _ = model.predict(X_batch)\n",
        "                    train_loss = np.mean(np.maximum(0, 1 - y_batch * train_pred))\n",
        "                    metrics['train_loss'].append(train_loss)\n",
        "\n",
        "                    # displaying progress\n",
        "                    display_epoch_summary(epoch, train_loss, val_results[f'class_{i}_accuracy'])\n",
        "                    # checking convergence\n",
        "                    if epoch > 15 and self.checking_convergence(state_metrics_history):\n",
        "                        print(\"Training converged - stopping early\")\n",
        "                        break\n",
        "        # evaluating finally\n",
        "        final_results, _ = self.validating_with_uncertainty(X_val, y_val)\n",
        "        print(f\"\\nTotal training time: {time.time() - start_time:.2f} seconds\")\n",
        "        display_final_results(final_results)\n",
        "        return final_results\n",
        "    def validating_per_state(self, predictions, true_labels):\n",
        "        metrics = {}\n",
        "        states = ['Helix', 'Sheet', 'Coil']\n",
        "        for state in states:\n",
        "            idx = states.index(state)\n",
        "            mask = true_labels == idx\n",
        "            if np.sum(mask) > 0:\n",
        "                accuracy = np.mean(predictions[mask] == true_labels[mask])\n",
        "                tp = np.sum((predictions == idx) & (true_labels == idx))\n",
        "                fp = np.sum((predictions == idx) & (true_labels != idx))\n",
        "                fn = np.sum((predictions != idx) & (true_labels == idx))\n",
        "\n",
        "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "                metrics[state] = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1\n",
        "                }\n",
        "            else:\n",
        "                metrics[state] = {\n",
        "                    'accuracy': 0.0,\n",
        "                    'precision': 0.0,\n",
        "                    'recall': 0.0,\n",
        "                    'f1': 0.0\n",
        "                }\n",
        "        return metrics\n",
        "\n",
        "    def predict(self, sequence):\n",
        "        features, _ = self.extracting_features(sequence)\n",
        "        X_scaled = self.scaler.transform(features)\n",
        "        # getting the predictions with the confidence scores\n",
        "        predictions = []\n",
        "        confidences = []\n",
        "        for model in self.svm_models:\n",
        "            pred, conf = model.predict(X_scaled)\n",
        "            predictions.append(pred)\n",
        "            confidences.append(conf)\n",
        "        predictions = np.array(predictions)\n",
        "        confidences = np.array(confidences)\n",
        "        # predicting weights by confidence and the class weights\n",
        "        class_weights = np.array([1.5, 6.0, 8.0])\n",
        "        confidence_scaling = np.power(confidences, 0.5)\n",
        "        weighted_predictions = predictions * confidence_scaling * class_weights[:, np.newaxis]\n",
        "        return np.argmax(weighted_predictions, axis=0)\n",
        "\n",
        "def analyzing_state_distribution(y_true, y_pred, states=['Helix', 'Sheet', 'Coil']):\n",
        "    metrics = {}\n",
        "    for i, state in enumerate(states):\n",
        "        state_mask = (y_true == i)\n",
        "        tp = np.sum((y_true == i) & (y_pred == i))\n",
        "        fp = np.sum((y_true != i) & (y_pred == i))\n",
        "        fn = np.sum((y_true == i) & (y_pred != i))\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "        metrics[state] = {\n",
        "            'accuracy': np.mean(y_true[state_mask] == y_pred[state_mask]),\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "    display_metrics_table(metrics)\n",
        "    return metrics\n",
        "\n",
        "def loading_and_processing_data():\n",
        "    try:\n",
        "        print(\"Loading data...\")\n",
        "        file_path = \"/content/drive/MyDrive/CB513.npy\"\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n",
        "\n",
        "        data = np.load(file_path, allow_pickle=True)\n",
        "        print(f\"Raw data shape: {data.shape}\")\n",
        "\n",
        "        # extracting the features and labels\n",
        "        data = data.reshape(-1, 700, 57)\n",
        "        features = data[:, :, :21].astype(np.float32)\n",
        "        labels = np.argmax(data[:, :, 42:45], axis=2)\n",
        "\n",
        "        print(f\"Features shape: {features.shape}\")\n",
        "        print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "        # analyzing the class distribution\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        for label, count in zip(unique, counts):\n",
        "            print(f\"Class {label}: {count} ({count/labels.size*100:.2f}%)\")\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    features, labels = loading_and_processing_data()\n",
        "    if features is None:\n",
        "        return None, None\n",
        "\n",
        "    train_seqs, test_seqs, train_labels, test_labels = train_test_split(\n",
        "        features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    predictor = ProteinStructurePredictor()\n",
        "    predictor.train(train_seqs, train_labels)\n",
        "\n",
        "    print(\"\\n Predictions Generated...\")\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    for seq, labels in zip(test_seqs, test_labels):\n",
        "        pred = predictor.predict(seq)\n",
        "        all_predictions.extend(pred)\n",
        "        all_true_labels.extend(labels)\n",
        "\n",
        "    analyzing_state_distribution(np.array(all_true_labels), np.array(all_predictions))\n",
        "    return predictor\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    predictor = main()"
      ]
    }
  ]
}